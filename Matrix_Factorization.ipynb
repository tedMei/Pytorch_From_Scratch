{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# add seed to make sure the result can be reproduced\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_matrix = torch.rand((200,300), dtype=torch.double) * 5 # 200 users and 300 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6612, 4.1210, 0.3640,  ..., 0.6650, 0.2198, 4.8357],\n",
       "        [1.5374, 0.6454, 3.7844,  ..., 0.6718, 1.7241, 4.8555],\n",
       "        [1.3164, 3.8213, 3.7675,  ..., 0.5212, 4.7950, 1.1560],\n",
       "        ...,\n",
       "        [2.2500, 0.3971, 3.9112,  ..., 1.5492, 1.6214, 0.0902],\n",
       "        [0.9362, 4.7794, 0.5447,  ..., 0.8912, 1.5400, 0.5561],\n",
       "        [3.7536, 2.4913, 2.9639,  ..., 4.7032, 3.6347, 3.5916]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_matrix = (torch.rand((200, 300)) > 0.2).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 1],\n",
       "        [1, 1, 0,  ..., 0, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 1, 0, 1],\n",
       "        [1, 0, 1,  ..., 0, 0, 1],\n",
       "        [1, 1, 0,  ..., 1, 0, 1]], dtype=torch.int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 200\n",
    "num_items = 300\n",
    "hidden_size = 30\n",
    "u = torch.randn(1, requires_grad = True)\n",
    "bu = torch.randn((num_users, 1), requires_grad = True)\n",
    "bi = torch.randn((num_items, 1), requires_grad = True)\n",
    "p = torch.randn((num_users, hidden_size), requires_grad = True)\n",
    "q = torch.randn((num_items, hidden_size), requires_grad = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 1706722.4886925956\n",
      "epoch 1000 loss 91196.75096977068\n",
      "epoch 2000 loss 82189.63200047139\n",
      "epoch 3000 loss 78685.08442425553\n",
      "epoch 4000 loss 75886.85518920128\n",
      "epoch 5000 loss 73485.60639042467\n",
      "epoch 6000 loss 71425.21193201157\n",
      "epoch 7000 loss 69651.54818728345\n",
      "epoch 8000 loss 68114.6049016031\n",
      "epoch 9000 loss 66773.89924696015\n",
      "epoch 10000 loss 65597.21150961949\n",
      "epoch 11000 loss 64558.55720098704\n",
      "epoch 12000 loss 63636.838692317855\n",
      "epoch 13000 loss 62814.934171483896\n",
      "epoch 14000 loss 62078.962231945734\n",
      "epoch 15000 loss 61417.616311114914\n",
      "epoch 16000 loss 60821.60424308446\n",
      "epoch 17000 loss 60283.1928522414\n",
      "epoch 18000 loss 59795.85800752274\n",
      "epoch 19000 loss 59354.027146591834\n",
      "epoch 20000 loss 58952.87739928913\n",
      "epoch 21000 loss 58588.190657962674\n",
      "epoch 22000 loss 58256.24244250346\n",
      "epoch 23000 loss 57953.7135069488\n",
      "epoch 24000 loss 57677.63088178211\n",
      "epoch 25000 loss 57425.32031735996\n",
      "epoch 26000 loss 57194.37234556438\n",
      "epoch 27000 loss 56982.61384577702\n",
      "epoch 28000 loss 56788.08712545064\n",
      "epoch 29000 loss 56609.034096430056\n",
      "epoch 30000 loss 56443.87930398318\n",
      "epoch 31000 loss 56291.214585497655\n",
      "epoch 32000 loss 56149.78642656064\n",
      "epoch 33000 loss 56018.48054541495\n",
      "epoch 34000 loss 55896.309422876235\n",
      "epoch 35000 loss 55782.39738415975\n",
      "epoch 36000 loss 55675.971390984785\n",
      "epoch 37000 loss 55576.34532728826\n",
      "epoch 38000 loss 55482.912512146824\n",
      "epoch 39000 loss 55395.13448624624\n",
      "epoch 40000 loss 55312.53348734934\n",
      "epoch 41000 loss 55234.684198281975\n",
      "epoch 42000 loss 55161.20661049978\n",
      "epoch 43000 loss 55091.75925386399\n",
      "epoch 44000 loss 55026.035634191234\n",
      "epoch 45000 loss 54963.75760280081\n",
      "epoch 46000 loss 54904.67398403326\n",
      "epoch 47000 loss 54848.55612398842\n",
      "epoch 48000 loss 54795.19233450021\n",
      "epoch 49000 loss 54744.390320800994\n",
      "epoch 50000 loss 54695.97277511265\n",
      "epoch 51000 loss 54649.7758458263\n",
      "epoch 52000 loss 54605.64799465357\n",
      "epoch 53000 loss 54563.45059679473\n",
      "epoch 54000 loss 54523.05382033854\n",
      "epoch 55000 loss 54484.339487297584\n",
      "epoch 56000 loss 54447.198063923366\n",
      "epoch 57000 loss 54411.52837448919\n",
      "epoch 58000 loss 54377.239799379066\n",
      "epoch 59000 loss 54344.24677379313\n",
      "epoch 60000 loss 54312.47208679531\n",
      "epoch 61000 loss 54281.84762494926\n",
      "epoch 62000 loss 54252.30932253985\n",
      "epoch 63000 loss 54223.798794927345\n",
      "epoch 64000 loss 54196.264808139815\n",
      "epoch 65000 loss 54169.65937294805\n",
      "epoch 66000 loss 54143.939278364014\n",
      "epoch 67000 loss 54119.06589742036\n",
      "epoch 68000 loss 54095.00163379311\n",
      "epoch 69000 loss 54071.71403266135\n",
      "epoch 70000 loss 54049.17240934055\n",
      "epoch 71000 loss 54027.34746005353\n",
      "epoch 72000 loss 54006.21308547199\n",
      "epoch 73000 loss 53985.74346791449\n",
      "epoch 74000 loss 53965.91365089419\n",
      "epoch 75000 loss 53946.70120028497\n",
      "epoch 76000 loss 53928.083839838364\n",
      "epoch 77000 loss 53910.03981478588\n",
      "epoch 78000 loss 53892.54769014679\n",
      "epoch 79000 loss 53875.58711514366\n",
      "epoch 80000 loss 53859.13898280081\n",
      "epoch 81000 loss 53843.18338946762\n",
      "epoch 82000 loss 53827.701972105584\n",
      "epoch 83000 loss 53812.674723056014\n",
      "epoch 84000 loss 53798.08439265641\n",
      "epoch 85000 loss 53783.912845259714\n",
      "epoch 86000 loss 53770.14412563293\n",
      "epoch 87000 loss 53756.76040915184\n",
      "epoch 88000 loss 53743.74595033122\n",
      "epoch 89000 loss 53731.08457045516\n",
      "epoch 90000 loss 53718.76194892303\n",
      "epoch 91000 loss 53706.76248186115\n",
      "epoch 92000 loss 53695.07243864451\n",
      "epoch 93000 loss 53683.677635536646\n",
      "epoch 94000 loss 53672.56562082911\n",
      "epoch 95000 loss 53661.72295344891\n",
      "epoch 96000 loss 53651.13772655791\n",
      "epoch 97000 loss 53640.797409645915\n",
      "epoch 98000 loss 53630.69140642065\n",
      "epoch 99000 loss 53620.809116039134\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100000):\n",
    "    \n",
    "    r_predict = torch.mm(p, q.t()) + bu + bi.t() + u # (num_users, num_items)\n",
    "    \n",
    "    loss = torch.sum((r_predict * mask_matrix - rating_matrix * mask_matrix).pow(2)) + 0.1 * (bu.pow(2).sum() + bi.pow(2).sum() + p.pow(2).sum() + q.pow(2).sum())\n",
    "    if epoch % 1000 == 0:\n",
    "        print(\"epoch\", epoch, \"loss\", loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        u -= learning_rate * u.grad\n",
    "        bu -= learning_rate * bu.grad\n",
    "        bi -= learning_rate * bi.grad\n",
    "        p -= learning_rate * p.grad\n",
    "        q -= learning_rate * q.grad\n",
    "        u.grad.zero_()\n",
    "        bu.grad.zero_()\n",
    "        bi.grad.zero_()\n",
    "        p.grad.zero_()\n",
    "        q.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    r_predict = torch.mm(p, q.t()) + bu + bi.t() + u # (num_users, num_items)\n",
    "    loss = torch.sum((r_predict * (1 - mask_matrix) - rating_matrix * (1-mask_matrix)).pow(2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45888.5589, dtype=torch.float64, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 3.3125, 4.0608, 0.0000],\n",
       "        [0.0000, 0.0000, 2.6633,  ..., 1.4139, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 3.5819, 0.0000],\n",
       "        [0.0000, 2.0817, 0.0000,  ..., 0.5112, 2.9696, 0.0000],\n",
       "        [0.0000, 0.0000, 2.2950,  ..., 0.0000, 3.2472, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_predict * (1 - mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.6650, 0.2198, 0.0000],\n",
       "        [0.0000, 0.0000, 3.7844,  ..., 0.6718, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.6214, 0.0000],\n",
       "        [0.0000, 4.7794, 0.0000,  ..., 0.8912, 1.5400, 0.0000],\n",
       "        [0.0000, 0.0000, 2.9639,  ..., 0.0000, 3.6347, 0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_matrix * (1-mask_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

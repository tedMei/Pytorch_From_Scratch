{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of https://arxiv.org/pdf/1508.04025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple dataset that does en-cn translation\n",
    "def load_data(in_file):\n",
    "    cn = []\n",
    "    en = []\n",
    "    num_examples = 0\n",
    "    with open(in_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(\"\\t\")\n",
    "            \n",
    "            en.append([\"BOS\"] + nltk.word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "            # split chinese sentence into characters for simplification\n",
    "            cn.append([\"BOS\"] + [c for c in line[1]] + [\"EOS\"])\n",
    "    return en, cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./nmt/en-cn/train.txt\"\n",
    "dev_file = \"./nmt/en-cn/dev.txt\"\n",
    "train_en, train_cn = load_data(train_file)\n",
    "dev_en, dev_cn = load_data(dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'],\n",
       " ['BOS', 'how', 'about', 'another', 'piece', 'of', 'cake', '?', 'EOS']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['BOS', '任', '何', '人', '都', '可', '以', '做', '到', '。', 'EOS'],\n",
       " ['BOS', '要', '不', '要', '再', '來', '一', '塊', '蛋', '糕', '？', 'EOS']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cn[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "UNK_IDX = 0\n",
    "PAD_IDX = 1\n",
    "\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    word_count = Counter()\n",
    "    for sentence in sentences:\n",
    "        for s in sentence:\n",
    "            word_count[s] += 1\n",
    "    vocab = word_count.most_common(max_words) # (word, count)\n",
    "    total_words = len(vocab) + 2 # plus UNK and PAD\n",
    "    word_dict = {w[0]: index+2 for index, w in enumerate(vocab)}\n",
    "    word_dict[\"UNK\"] = UNK_IDX\n",
    "    word_dict[\"PAD\"] = PAD_IDX\n",
    "    return word_dict, total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict, en_total_words = build_dict(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_dict, cn_total_words = build_dict(train_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_en_dict = {v: k for k, v in en_dict.items()}\n",
    "inv_cn_dict = {v: k for k, v in cn_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode sentences into numbers\n",
    "def encode(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len = True):\n",
    "    \n",
    "    length = len(en_sentences)\n",
    "    out_en_sentences = [[en_dict.get(w, 0) for w in sen] for sen in en_sentences]\n",
    "    out_cn_sentences = [[cn_dict.get(w, 0) for w in sen] for sen in cn_sentences]\n",
    "    \n",
    "    # sort sentences by english length to make sure each batch has similar length\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key = lambda x : len(seq[x])) # index of smaller length comes first\n",
    "    \n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_cn_sentences = [out_cn_sentences[i] for i in sorted_index]\n",
    "    \n",
    "    return out_en_sentences, out_cn_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)\n",
    "dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS\n",
      "BOS for what purpose did he come here ? EOS\n"
     ]
    }
   ],
   "source": [
    "# checking\n",
    "k = 10000\n",
    "print(\" \".join([inv_cn_dict[i] for i in train_cn[k]]))\n",
    "print(\" \".join([inv_en_dict[i] for i in train_en[k]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx+minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n",
       " array([60, 61, 62, 63, 64, 65, 66, 67, 68, 69]),\n",
       " array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n",
       " array([90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([70, 71, 72, 73, 74, 75, 76, 77, 78, 79]),\n",
       " array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n",
       " array([80, 81, 82, 83, 84, 85, 86, 87, 88, 89]),\n",
       " array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n",
       " array([50, 51, 52, 53, 54, 55, 56, 57, 58, 59])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding to every sentence and make every sentence have the same length\n",
    "def prepare_data(seqs):\n",
    "    lengths = [len(seq) for seq in seqs]\n",
    "    n_samples = len(seqs)\n",
    "    max_len = np.max(lengths)\n",
    "    \n",
    "    x = np.zeros((n_samples, max_len)).astype('int32')\n",
    "    x_lengths = np.array(lengths).astype('int32')\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        x[idx, :lengths[idx]] = seq\n",
    "    return x, x_lengths # sentences after padding, actual length before padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_examples(en_sentences, cn_sentences, batch_size):\n",
    "    minibatches = get_minibatches(len(en_sentences), batch_size)\n",
    "    all_examples = []\n",
    "    for minibatch in minibatches:\n",
    "        mb_en_sentences = [en_sentences[t] for t in minibatch]\n",
    "        mb_cn_sentences = [cn_sentences[t] for t in minibatch]\n",
    "        mb_x, mb_x_len = prepare_data(mb_en_sentences)\n",
    "        mb_y, mb_y_len = prepare_data(mb_cn_sentences)\n",
    "        all_examples.append((mb_x, mb_x_len, mb_y, mb_y_len))\n",
    "    return all_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = gen_examples(train_en, train_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "        8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]),\n",
       "indices=tensor([41, 40, 46, 45, 44, 43, 42, 48, 49, 39, 38, 37, 36, 35, 47, 33, 57, 56,\n",
       "        62, 61, 60, 59, 58, 31, 50, 55, 54, 53, 52, 51, 34,  0, 32,  8,  7, 13,\n",
       "        12, 11, 10,  9, 15, 16,  6,  5,  4,  3,  2, 14,  1, 24, 23, 29, 28, 27,\n",
       "        26, 25, 30, 17, 22, 21, 20, 19, 18, 63]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor(train_data[0][1]).sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = gen_examples(dev_en, dev_cn, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "        x: (batch_size, max_length_in_batch)\n",
    "        lengths: tensor of shape (batch_size)\n",
    "        '''\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True) # sorted_len: (batch_size), sorted_idx: (batch_size)\n",
    "        x_sorted = x[sorted_idx.long()] # every sentence is sorted from long to short in each batch now\n",
    "        embedded = self.dropout(self.embed(x_sorted)) # (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # avoid computing the hidden state for padding using pack_padded_sequence, this function has to sort sentence by length first\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded) # packed_out: (batch_size, max_length, hidden_size), hid: (1, batch, hidden_size)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) # out: (batch_size, max_length, hidden_size)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous() # out: (batch_size, max_length, hidden_size)\n",
    "        hid = hid[:,original_idx.long()].contiguous() # the : selects the first dimention which is 1, index works on batch dimension only\n",
    "        \n",
    "        return out, hid[[-1]] # hid[-1]: (batch_size, hidden_size), hid[[-1]]: (1, batch_size, hidden_size)\n",
    "\n",
    "\n",
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, dropout=0.2):\n",
    "        super(SimpleDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()] # (1, batch_size, max_length)\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))  # (batch_size, max_len, embed_size/hidden_size)\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True) \n",
    "        out, hid = self.rnn(packed_seq, hid) # out: (batch_size, max_length, hidden_size), hid: (1, batch, hidden_size)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous() # output_seq: (batch_size, max_length, hidden_size)\n",
    "        hid = hid[:, original_idx.long()].contiguous() # hid: (1, batch, hidden_size)\n",
    "\n",
    "        output = F.log_softmax(self.out(output_seq), -1) # (batch_size, max_length, vocab_size)\n",
    "        \n",
    "        return output, hid\n",
    "\n",
    "class SimpleSeq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(SimpleSeq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self,x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid = self.decoder(y, y_lengths, hid)\n",
    "        return output, None # placeholder for attention\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length = 10):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        batch_size = x.shape[0]\n",
    "        preds = []\n",
    "        for i in range(max_length):\n",
    "            output, hid = self.decoder(y, torch.ones(batch_size).long().to(y.device), hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "        return torch.cat(preds, 1), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked cross entropy loss\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        '''\n",
    "        input: (batch_size, max_len, vocab_size)\n",
    "        '''\n",
    "        input = input.contiguous().view(-1, input.size(2)) # (batch_size * max_len, vocab_size)\n",
    "        target = target.contiguous().view(-1, 1) # (batch_size * max_len, 1)\n",
    "        mask = mask.contiguous().view(-1, 1) # (batch_size * max_length, 1)\n",
    "        output = -input.gather(1, target) * mask # select the part in input where target is equal to 1 and then time mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout = 0.2\n",
    "hidden_size = 100\n",
    "encoder = SimpleEncoder(vocab_size=en_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = SimpleDecoder(vocab_size=cn_total_words,\n",
    "                      hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = SimpleSeq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    total_num_words = total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long()\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "\n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)\n",
    "\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None]\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "\n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "\n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "    print(\"Evaluation loss\", total_loss/total_num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 8.110105514526367\n",
      "Epoch 0 iteration 100 loss 5.299361228942871\n",
      "Epoch 0 iteration 200 loss 5.251104831695557\n",
      "Epoch 0 Training loss 5.4507395670388155\n",
      "Evaluation loss 4.8519430413628495\n",
      "Epoch 1 iteration 0 loss 4.411635875701904\n",
      "Epoch 1 iteration 100 loss 4.724141597747803\n",
      "Epoch 1 iteration 200 loss 4.8460564613342285\n",
      "Epoch 1 Training loss 4.62219145066978\n",
      "Epoch 2 iteration 0 loss 3.873622417449951\n",
      "Epoch 2 iteration 100 loss 4.337270736694336\n",
      "Epoch 2 iteration 200 loss 4.554322242736816\n",
      "Epoch 2 Training loss 4.231444733532659\n",
      "Epoch 3 iteration 0 loss 3.563133955001831\n",
      "Epoch 3 iteration 100 loss 4.088422775268555\n",
      "Epoch 3 iteration 200 loss 4.333175182342529\n",
      "Epoch 3 Training loss 3.9670103747909575\n",
      "Epoch 4 iteration 0 loss 3.3092098236083984\n",
      "Epoch 4 iteration 100 loss 3.8547275066375732\n",
      "Epoch 4 iteration 200 loss 4.176016330718994\n",
      "Epoch 4 Training loss 3.767932853786133\n",
      "Epoch 5 iteration 0 loss 3.1728157997131348\n",
      "Epoch 5 iteration 100 loss 3.6784474849700928\n",
      "Epoch 5 iteration 200 loss 4.039896011352539\n",
      "Epoch 5 Training loss 3.604061539283832\n",
      "Evaluation loss 3.6769536718477465\n",
      "Epoch 6 iteration 0 loss 3.016451835632324\n",
      "Epoch 6 iteration 100 loss 3.538015365600586\n",
      "Epoch 6 iteration 200 loss 3.951204299926758\n",
      "Epoch 6 Training loss 3.4717178772872392\n",
      "Epoch 7 iteration 0 loss 2.862421989440918\n",
      "Epoch 7 iteration 100 loss 3.404808521270752\n",
      "Epoch 7 iteration 200 loss 3.836179494857788\n",
      "Epoch 7 Training loss 3.358924897342367\n",
      "Epoch 8 iteration 0 loss 2.7641773223876953\n",
      "Epoch 8 iteration 100 loss 3.2853200435638428\n",
      "Epoch 8 iteration 200 loss 3.7472879886627197\n",
      "Epoch 8 Training loss 3.2600173245659696\n",
      "Epoch 9 iteration 0 loss 2.7008814811706543\n",
      "Epoch 9 iteration 100 loss 3.19246506690979\n",
      "Epoch 9 iteration 200 loss 3.6848537921905518\n",
      "Epoch 9 Training loss 3.1734553113223685\n",
      "Epoch 10 iteration 0 loss 2.5601797103881836\n",
      "Epoch 10 iteration 100 loss 3.1292881965637207\n",
      "Epoch 10 iteration 200 loss 3.6116902828216553\n",
      "Epoch 10 Training loss 3.0953022560979493\n",
      "Evaluation loss 3.390826309621045\n",
      "Epoch 11 iteration 0 loss 2.5111796855926514\n",
      "Epoch 11 iteration 100 loss 3.0527048110961914\n",
      "Epoch 11 iteration 200 loss 3.5396857261657715\n",
      "Epoch 11 Training loss 3.023419192446611\n",
      "Epoch 12 iteration 0 loss 2.4560482501983643\n",
      "Epoch 12 iteration 100 loss 2.9574167728424072\n",
      "Epoch 12 iteration 200 loss 3.4863903522491455\n",
      "Epoch 12 Training loss 2.9602284705301316\n",
      "Epoch 13 iteration 0 loss 2.4019649028778076\n",
      "Epoch 13 iteration 100 loss 2.8861701488494873\n",
      "Epoch 13 iteration 200 loss 3.46095871925354\n",
      "Epoch 13 Training loss 2.8998653280375284\n",
      "Epoch 14 iteration 0 loss 2.3648018836975098\n",
      "Epoch 14 iteration 100 loss 2.8772459030151367\n",
      "Epoch 14 iteration 200 loss 3.4220151901245117\n",
      "Epoch 14 Training loss 2.8441048391368713\n",
      "Epoch 15 iteration 0 loss 2.270256519317627\n",
      "Epoch 15 iteration 100 loss 2.7973246574401855\n",
      "Epoch 15 iteration 200 loss 3.3565807342529297\n",
      "Epoch 15 Training loss 2.793352776086493\n",
      "Evaluation loss 3.2659318848675487\n",
      "Epoch 16 iteration 0 loss 2.243673086166382\n",
      "Epoch 16 iteration 100 loss 2.7412259578704834\n",
      "Epoch 16 iteration 200 loss 3.325585126876831\n",
      "Epoch 16 Training loss 2.7454096335957865\n",
      "Epoch 17 iteration 0 loss 2.2205145359039307\n",
      "Epoch 17 iteration 100 loss 2.691185712814331\n",
      "Epoch 17 iteration 200 loss 3.3099772930145264\n",
      "Epoch 17 Training loss 2.699504980235463\n",
      "Epoch 18 iteration 0 loss 2.1762237548828125\n",
      "Epoch 18 iteration 100 loss 2.5928761959075928\n",
      "Epoch 18 iteration 200 loss 3.2317230701446533\n",
      "Epoch 18 Training loss 2.6568571728807457\n",
      "Epoch 19 iteration 0 loss 2.1631484031677246\n",
      "Epoch 19 iteration 100 loss 2.5953280925750732\n",
      "Epoch 19 iteration 200 loss 3.198646068572998\n",
      "Epoch 19 Training loss 2.618456271074295\n",
      "Epoch 20 iteration 0 loss 2.1029953956604004\n",
      "Epoch 20 iteration 100 loss 2.567213296890259\n",
      "Epoch 20 iteration 200 loss 3.1570727825164795\n",
      "Epoch 20 Training loss 2.581796749498129\n",
      "Evaluation loss 3.198751604016145\n",
      "Epoch 21 iteration 0 loss 2.048666477203369\n",
      "Epoch 21 iteration 100 loss 2.512131690979004\n",
      "Epoch 21 iteration 200 loss 3.137601613998413\n",
      "Epoch 21 Training loss 2.546957017950126\n",
      "Epoch 22 iteration 0 loss 2.046097755432129\n",
      "Epoch 22 iteration 100 loss 2.4737889766693115\n",
      "Epoch 22 iteration 200 loss 3.1367523670196533\n",
      "Epoch 22 Training loss 2.514069681942752\n",
      "Epoch 23 iteration 0 loss 1.9912587404251099\n",
      "Epoch 23 iteration 100 loss 2.4395101070404053\n",
      "Epoch 23 iteration 200 loss 3.0892157554626465\n",
      "Epoch 23 Training loss 2.478022837255309\n",
      "Epoch 24 iteration 0 loss 1.9366189241409302\n",
      "Epoch 24 iteration 100 loss 2.4071848392486572\n",
      "Epoch 24 iteration 200 loss 3.130948066711426\n",
      "Epoch 24 Training loss 2.450072027355161\n",
      "Epoch 25 iteration 0 loss 1.9149175882339478\n",
      "Epoch 25 iteration 100 loss 2.390409469604492\n",
      "Epoch 25 iteration 200 loss 3.0546352863311768\n",
      "Epoch 25 Training loss 2.4201172701470615\n",
      "Evaluation loss 3.1531791687741832\n",
      "Epoch 26 iteration 0 loss 1.8871681690216064\n",
      "Epoch 26 iteration 100 loss 2.364189624786377\n",
      "Epoch 26 iteration 200 loss 3.0297489166259766\n",
      "Epoch 26 Training loss 2.3900814908292562\n",
      "Epoch 27 iteration 0 loss 1.8735580444335938\n",
      "Epoch 27 iteration 100 loss 2.300553798675537\n",
      "Epoch 27 iteration 200 loss 3.029301404953003\n",
      "Epoch 27 Training loss 2.3621245395884434\n",
      "Epoch 28 iteration 0 loss 1.8384225368499756\n",
      "Epoch 28 iteration 100 loss 2.2999069690704346\n",
      "Epoch 28 iteration 200 loss 2.9460995197296143\n",
      "Epoch 28 Training loss 2.3344486888949514\n",
      "Epoch 29 iteration 0 loss 1.8350989818572998\n",
      "Epoch 29 iteration 100 loss 2.281726360321045\n",
      "Epoch 29 iteration 200 loss 2.9615113735198975\n",
      "Epoch 29 Training loss 2.311423004261378\n"
     ]
    }
   ],
   "source": [
    "def train(model, data, num_epochs = 20):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_num_words = total_loss = 0\n",
    "        for it, (mb_x, mb_x_len, mb_y, mb_y_len) in enumerate(data):\n",
    "            mb_x = torch.from_numpy(mb_x).to(device).long() # (batch_size, max_len)\n",
    "            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()\n",
    "            # input does not include the last word\n",
    "            mb_input = torch.from_numpy(mb_y[:, :-1]).to(device).long()\n",
    "            # expected output is one offset off compared with the mb_input\n",
    "            mb_output = torch.from_numpy(mb_y[:, 1:]).to(device).long()\n",
    "            mb_y_len = torch.from_numpy(mb_y_len-1).to(device).long()\n",
    "            \n",
    "            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len) # (batch_size, max_length, vocab_size)\n",
    "            \n",
    "            # (1, max_length) < (batch_size, 1) => (batch_size, max_length)\n",
    "            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[None, :] < mb_y_len[:, None] \n",
    "            # for each row, 1 indicates non-mask, 0 indicates the word is masked (the word is padding)\n",
    "            mb_out_mask = mb_out_mask.float()\n",
    "            \n",
    "            loss = loss_fn(mb_pred, mb_output, mb_out_mask)\n",
    "            \n",
    "            num_words = torch.sum(mb_y_len).item()\n",
    "            total_loss += loss.item() * num_words\n",
    "            total_num_words += num_words\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                print(\"Epoch\", epoch, \"iteration\", it, \"loss\", loss.item())\n",
    "                \n",
    "        print(\"Epoch\", epoch, \"Training loss\", total_loss/total_num_words)\n",
    "        if epoch % 5 == 0:\n",
    "            evaluate(model, dev_data)\n",
    "        \n",
    "train(model, train_data, num_epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有一个好。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你们现在了。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每个人都知道他的名字\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "在哪裡？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我們在那個男孩。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "这是你的手錶。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們在學校。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "這個房間有一個很好。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它太好。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "每个人都在笑。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "這個工作很好。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "你有很多人。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我的手臂斷了。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜欢阅读。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有任何人都来。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請把門關上。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆在这里。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請把英文歌。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "在下午餐。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我在一個月。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    en_sent = \" \".join([inv_en_dict[w] for w in dev_en[i]])\n",
    "    print(en_sent)\n",
    "    cn_sent = \" \".join([inv_cn_dict[w] for w in dev_cn[i]])\n",
    "    print(\"\".join(cn_sent))\n",
    "\n",
    "    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(1, -1)).long().to(device)\n",
    "    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)\n",
    "    bos = torch.Tensor([[cn_dict[\"BOS\"]]]).long().to(device)\n",
    "\n",
    "    translation, attn = model.translate(mb_x, mb_x_len, bos)\n",
    "    translation = [inv_cn_dict[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print(\"\".join(trans))\n",
    "\n",
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of https://arxiv.org/pdf/1508.04025.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        '''\n",
    "        x: (batch_size, max_length_in_batch)\n",
    "        lengths: tensor of shape (batch_size)\n",
    "        '''\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True) # sorted_len: (batch_size), sorted_idx: (batch_size)\n",
    "        x_sorted = x[sorted_idx.long()] # every sentence is sorted from long to short in each batch now\n",
    "        embedded = self.dropout(self.embed(x_sorted)) # (batch_size, max_length, embed_size)\n",
    "        \n",
    "        # avoid computing the hidden state for padding using pack_padded_sequence, this function has to sort sentence by length first\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        packed_out, hid = self.rnn(packed_embedded) # packed_out: (batch_size, max_length, enc_hidden_size), hid: (2, batch, enc_hidden_size)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) # out: (batch_size, max_length, enc_hidden_size)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous() # out: (batch_size, max_length, enc_hidden_size)\n",
    "        hid = hid[:,original_idx.long()].contiguous() # the : selects the first dimention which is 2, index works on batch dimension only\n",
    "        \n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim = -1) # (batch_size, 2 * enc_hidden_size)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0) # (1, batch_size, dec_hidden_size)\n",
    "        return out, hid\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "        \n",
    "    def forward(self, output, context, mask):\n",
    "        '''\n",
    "        output: the hidden output of each cell in the decoder, (batch_size, output_len, dec_hidden_size) -> notice in the training step, we feed in actual translated word. So we can compute hidden state for each word rather than feed one by one sequentially\n",
    "        context: the out of encoder, (batch_size, context_len, 2 * enc_hidden_size)\n",
    "        '''\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "        \n",
    "        context_in = self.linear_in(context.view(batch_size*input_len, -1)).view(                \n",
    "            batch_size, input_len, -1) # (batch_size, context_len, dec_hidden_size) -> this is to do W*hs, next step is to dot product with ht (the hidden state from decoder)\n",
    "        \n",
    "        attn = torch.bmm(output, context_in.transpose(1,2)) # (batch_size, output_len, context_len)\n",
    "\n",
    "        attn.data.masked_fill(mask, -1e6) # minimize the effect of padding\n",
    "\n",
    "        attn = F.softmax(attn, dim=2)  # (batch_size, output_len, context_len)\n",
    "\n",
    "        context = torch.bmm(attn, context) # (batch_size, output_len, 2 * enc_hidden_size)\n",
    "        \n",
    "        output = torch.cat((context, output), dim=2) # (batch_size, output_len, 2 * enc_hidden_size + dec_hidden_size)\n",
    "\n",
    "        output = output.view(batch_size*output_len, -1) # (batch_size * output_len, 2 * enc_hidden_size + dec_hidden_size)\n",
    "        output = torch.tanh(self.linear_out(output)) # (batch_size * output_len, dec_hidden_size)\n",
    "        output = output.view(batch_size, output_len, -1) # (batch_size, output_len, dec_hidden_size)\n",
    "        return output, attn\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, dec_hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        device = x_len.device\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = (torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None])\n",
    "        x_mask = x_mask.float()\n",
    "        y_mask = (torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None])\n",
    "        y_mask = y_mask.float()\n",
    "        mask = (1 - x_mask[:, :, None] * y_mask[:, None, :]).byte()\n",
    "        return mask # (batch_size, x_len, y_len)\n",
    "    \n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()] # (1, batch_size, max_length)\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))  # (batch_size, max_len, embed_size)\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True) \n",
    "        out, hid = self.rnn(packed_seq, hid) # out: (batch_size, max_length, hidden_size), hid: (1, batch, hidden_size)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous() # output_seq: (batch_size, max_length, hidden_size)\n",
    "        hid = hid[:, original_idx.long()].contiguous() # hid: (1, batch, hidden_size)\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1) # (batch_size, max_length, vocab_size)\n",
    "        \n",
    "        return output, hid, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)\n",
    "        return output, attn\n",
    "    \n",
    "    def translate(self, x, x_lengths, y, max_length=100):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out, \n",
    "                    ctx_lengths=x_lengths,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "embed_size = hidden_size = 100\n",
    "encoder = Encoder(vocab_size=en_total_words,\n",
    "                       embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "decoder = Decoder(vocab_size=cn_total_words,\n",
    "                      embed_size=embed_size,\n",
    "                      enc_hidden_size=hidden_size,\n",
    "                       dec_hidden_size=hidden_size,\n",
    "                      dropout=dropout)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "model = model.to(device)\n",
    "loss_fn = LanguageModelCriterion().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 iteration 0 loss 2.5341858863830566\n",
      "Epoch 0 iteration 100 loss 3.253624439239502\n",
      "Epoch 0 iteration 200 loss 3.719940423965454\n",
      "Epoch 0 Training loss 3.1440185637054\n",
      "Evaluation loss 3.361748992043007\n",
      "Epoch 1 iteration 0 loss 2.454204797744751\n",
      "Epoch 1 iteration 100 loss 3.115828275680542\n",
      "Epoch 1 iteration 200 loss 3.635378122329712\n",
      "Epoch 1 Training loss 3.039223692395219\n",
      "Epoch 2 iteration 0 loss 2.367974281311035\n",
      "Epoch 2 iteration 100 loss 3.0356693267822266\n",
      "Epoch 2 iteration 200 loss 3.524312734603882\n",
      "Epoch 2 Training loss 2.9415965439987297\n",
      "Epoch 3 iteration 0 loss 2.325373649597168\n",
      "Epoch 3 iteration 100 loss 2.894467353820801\n",
      "Epoch 3 iteration 200 loss 3.4648382663726807\n",
      "Epoch 3 Training loss 2.859146160381086\n",
      "Epoch 4 iteration 0 loss 2.252025842666626\n",
      "Epoch 4 iteration 100 loss 2.8076364994049072\n",
      "Epoch 4 iteration 200 loss 3.3692665100097656\n",
      "Epoch 4 Training loss 2.7840369354817716\n",
      "Epoch 5 iteration 0 loss 2.193591833114624\n",
      "Epoch 5 iteration 100 loss 2.7523467540740967\n",
      "Epoch 5 iteration 200 loss 3.2956466674804688\n",
      "Epoch 5 Training loss 2.705976186393898\n",
      "Evaluation loss 3.142341250148647\n",
      "Epoch 6 iteration 0 loss 2.0900442600250244\n",
      "Epoch 6 iteration 100 loss 2.6301708221435547\n",
      "Epoch 6 iteration 200 loss 3.2766315937042236\n",
      "Epoch 6 Training loss 2.640709365502102\n",
      "Epoch 7 iteration 0 loss 2.0405144691467285\n",
      "Epoch 7 iteration 100 loss 2.6125471591949463\n",
      "Epoch 7 iteration 200 loss 3.179985761642456\n",
      "Epoch 7 Training loss 2.5786414175664794\n",
      "Epoch 8 iteration 0 loss 2.0158345699310303\n",
      "Epoch 8 iteration 100 loss 2.5640316009521484\n",
      "Epoch 8 iteration 200 loss 3.112520694732666\n",
      "Epoch 8 Training loss 2.5193043620479343\n",
      "Epoch 9 iteration 0 loss 1.9402087926864624\n",
      "Epoch 9 iteration 100 loss 2.453336238861084\n",
      "Epoch 9 iteration 200 loss 3.0767180919647217\n",
      "Epoch 9 Training loss 2.462849580266493\n",
      "Epoch 10 iteration 0 loss 1.8870643377304077\n",
      "Epoch 10 iteration 100 loss 2.370480537414551\n",
      "Epoch 10 iteration 200 loss 3.06303071975708\n",
      "Epoch 10 Training loss 2.41383097959558\n",
      "Evaluation loss 3.0476606723050614\n",
      "Epoch 11 iteration 0 loss 1.835741639137268\n",
      "Epoch 11 iteration 100 loss 2.3508687019348145\n",
      "Epoch 11 iteration 200 loss 2.971958637237549\n",
      "Epoch 11 Training loss 2.362232419283157\n",
      "Epoch 12 iteration 0 loss 1.7861202955245972\n",
      "Epoch 12 iteration 100 loss 2.3111140727996826\n",
      "Epoch 12 iteration 200 loss 2.962618827819824\n",
      "Epoch 12 Training loss 2.3174705233399346\n",
      "Epoch 13 iteration 0 loss 1.7212458848953247\n",
      "Epoch 13 iteration 100 loss 2.2093589305877686\n",
      "Epoch 13 iteration 200 loss 2.9052860736846924\n",
      "Epoch 13 Training loss 2.2712704652880165\n",
      "Epoch 14 iteration 0 loss 1.7076150178909302\n",
      "Epoch 14 iteration 100 loss 2.218489408493042\n",
      "Epoch 14 iteration 200 loss 2.839616298675537\n",
      "Epoch 14 Training loss 2.232854853262958\n",
      "Epoch 15 iteration 0 loss 1.6283740997314453\n",
      "Epoch 15 iteration 100 loss 2.1836493015289307\n",
      "Epoch 15 iteration 200 loss 2.8350043296813965\n",
      "Epoch 15 Training loss 2.1909997641640384\n",
      "Evaluation loss 3.004113322918359\n",
      "Epoch 16 iteration 0 loss 1.6071207523345947\n",
      "Epoch 16 iteration 100 loss 2.09377121925354\n",
      "Epoch 16 iteration 200 loss 2.7369906902313232\n",
      "Epoch 16 Training loss 2.1529491149957445\n",
      "Epoch 17 iteration 0 loss 1.6219902038574219\n",
      "Epoch 17 iteration 100 loss 2.0534348487854004\n",
      "Epoch 17 iteration 200 loss 2.79337739944458\n",
      "Epoch 17 Training loss 2.1169144270139957\n",
      "Epoch 18 iteration 0 loss 1.5528156757354736\n",
      "Epoch 18 iteration 100 loss 2.06282377243042\n",
      "Epoch 18 iteration 200 loss 2.724085807800293\n",
      "Epoch 18 Training loss 2.082609166674178\n",
      "Epoch 19 iteration 0 loss 1.4947049617767334\n",
      "Epoch 19 iteration 100 loss 2.0231070518493652\n",
      "Epoch 19 iteration 200 loss 2.7106246948242188\n",
      "Epoch 19 Training loss 2.051894315390788\n",
      "Epoch 20 iteration 0 loss 1.4527027606964111\n",
      "Epoch 20 iteration 100 loss 1.9787992238998413\n",
      "Epoch 20 iteration 200 loss 2.6644046306610107\n",
      "Epoch 20 Training loss 2.015939031562427\n",
      "Evaluation loss 2.984301816387076\n",
      "Epoch 21 iteration 0 loss 1.4461777210235596\n",
      "Epoch 21 iteration 100 loss 1.9214411973953247\n",
      "Epoch 21 iteration 200 loss 2.5992672443389893\n",
      "Epoch 21 Training loss 1.988347126033163\n",
      "Epoch 22 iteration 0 loss 1.4068058729171753\n",
      "Epoch 22 iteration 100 loss 1.865005612373352\n",
      "Epoch 22 iteration 200 loss 2.5915515422821045\n",
      "Epoch 22 Training loss 1.957012974915485\n",
      "Epoch 23 iteration 0 loss 1.403914213180542\n",
      "Epoch 23 iteration 100 loss 1.8399031162261963\n",
      "Epoch 23 iteration 200 loss 2.5728538036346436\n",
      "Epoch 23 Training loss 1.9286541976439942\n",
      "Epoch 24 iteration 0 loss 1.3819372653961182\n",
      "Epoch 24 iteration 100 loss 1.8574453592300415\n",
      "Epoch 24 iteration 200 loss 2.5197155475616455\n",
      "Epoch 24 Training loss 1.9035191670846046\n",
      "Epoch 25 iteration 0 loss 1.3633137941360474\n",
      "Epoch 25 iteration 100 loss 1.768286943435669\n",
      "Epoch 25 iteration 200 loss 2.554229974746704\n",
      "Epoch 25 Training loss 1.876052114408745\n",
      "Evaluation loss 2.9892975742647905\n",
      "Epoch 26 iteration 0 loss 1.3335778713226318\n",
      "Epoch 26 iteration 100 loss 1.7437289953231812\n",
      "Epoch 26 iteration 200 loss 2.4179084300994873\n",
      "Epoch 26 Training loss 1.847099374533718\n",
      "Epoch 27 iteration 0 loss 1.2823172807693481\n",
      "Epoch 27 iteration 100 loss 1.7135545015335083\n",
      "Epoch 27 iteration 200 loss 2.4941999912261963\n",
      "Epoch 27 Training loss 1.8225131431014763\n",
      "Epoch 28 iteration 0 loss 1.3373029232025146\n",
      "Epoch 28 iteration 100 loss 1.7501072883605957\n",
      "Epoch 28 iteration 200 loss 2.430213451385498\n",
      "Epoch 28 Training loss 1.7996671704623672\n",
      "Epoch 29 iteration 0 loss 1.2481900453567505\n",
      "Epoch 29 iteration 100 loss 1.650691032409668\n",
      "Epoch 29 iteration 200 loss 2.4200847148895264\n",
      "Epoch 29 Training loss 1.775445291062462\n"
     ]
    }
   ],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "train(model, train_data, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS you have nice skin . EOS\n",
      "BOS 你 的 皮 膚 真 好 。 EOS\n",
      "你有很苍白。\n",
      "\n",
      "BOS you 're UNK correct . EOS\n",
      "BOS 你 部 分 正 确 。 EOS\n",
      "你真的是重要的。\n",
      "\n",
      "BOS everyone admired his courage . EOS\n",
      "BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS\n",
      "每個人都認識他的。\n",
      "\n",
      "BOS what time is it ? EOS\n",
      "BOS 几 点 了 ？ EOS\n",
      "它什么时候到？\n",
      "\n",
      "BOS i 'm free tonight . EOS\n",
      "BOS 我 今 晚 有 空 。 EOS\n",
      "我今晚有好。\n",
      "\n",
      "BOS here is your book . EOS\n",
      "BOS 這 是 你 的 書 。 EOS\n",
      "你的書在書。\n",
      "\n",
      "BOS they are at lunch . EOS\n",
      "BOS 他 们 在 吃 午 饭 。 EOS\n",
      "他們正在吃午餐。\n",
      "\n",
      "BOS this chair is UNK . EOS\n",
      "BOS 這 把 椅 子 很 UNK 。 EOS\n",
      "這件衣服。\n",
      "\n",
      "BOS it 's pretty heavy . EOS\n",
      "BOS 它 真 重 。 EOS\n",
      "它是一樣的。\n",
      "\n",
      "BOS many attended his funeral . EOS\n",
      "BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS\n",
      "他的父親都是他的。\n",
      "\n",
      "BOS training will be provided . EOS\n",
      "BOS 会 有 训 练 。 EOS\n",
      "鬱金會將會議。\n",
      "\n",
      "BOS someone is watching you . EOS\n",
      "BOS 有 人 在 看 著 你 。 EOS\n",
      "有人在看你。\n",
      "\n",
      "BOS i slapped his face . EOS\n",
      "BOS 我 摑 了 他 的 臉 。 EOS\n",
      "我把他的手錶了。\n",
      "\n",
      "BOS i like UNK music . EOS\n",
      "BOS 我 喜 歡 流 行 音 樂 。 EOS\n",
      "我喜欢音樂。\n",
      "\n",
      "BOS tom had no children . EOS\n",
      "BOS T o m 沒 有 孩 子 。 EOS\n",
      "汤姆没有人都不能看。\n",
      "\n",
      "BOS please lock the door . EOS\n",
      "BOS 請 把 門 鎖 上 。 EOS\n",
      "請關門。\n",
      "\n",
      "BOS tom has calmed down . EOS\n",
      "BOS 汤 姆 冷 静 下 来 了 。 EOS\n",
      "汤姆做了个。\n",
      "\n",
      "BOS please speak more loudly . EOS\n",
      "BOS 請 說 大 聲 一 點 兒 。 EOS\n",
      "請說更多。\n",
      "\n",
      "BOS keep next sunday free . EOS\n",
      "BOS 把 下 周 日 空 出 来 。 EOS\n",
      "下周下雨。\n",
      "\n",
      "BOS i made a mistake . EOS\n",
      "BOS 我 犯 了 一 個 錯 。 EOS\n",
      "我犯了一個錯誤。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,120):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# add seed to make sure the result can be reproduced\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "C = 3 # window size\n",
    "K = 100 # number of negative samples\n",
    "NUM_EPOCHS = 2\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.1\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"text/text.train.txt\", \"r\") as fin:\n",
    "    text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [w for w in word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1)) # key: word ï¼› value: count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values())) # put the words other than top MAX_VOCAB_SIZE - 1 to UNK. Count the number occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 958035,\n",
       " 'of': 536684,\n",
       " 'and': 375233,\n",
       " 'one': 371796,\n",
       " 'in': 335503,\n",
       " 'a': 292250,\n",
       " 'to': 285093,\n",
       " 'zero': 235406,\n",
       " 'nine': 224705,\n",
       " 'two': 172079,\n",
       " 'is': 164575,\n",
       " 'as': 118931,\n",
       " 'eight': 113412,\n",
       " 'for': 106452,\n",
       " 's': 104935,\n",
       " 'five': 104416,\n",
       " 'three': 103344,\n",
       " 'was': 101939,\n",
       " 'by': 100587,\n",
       " 'that': 98710,\n",
       " 'four': 97719,\n",
       " 'six': 91897,\n",
       " 'seven': 90940,\n",
       " 'with': 85741,\n",
       " 'on': 82022,\n",
       " 'are': 68769,\n",
       " 'it': 66093,\n",
       " 'from': 65738,\n",
       " 'or': 62113,\n",
       " 'his': 58234,\n",
       " 'an': 55741,\n",
       " 'be': 55471,\n",
       " 'this': 53073,\n",
       " 'he': 49838,\n",
       " 'at': 49636,\n",
       " 'which': 49410,\n",
       " 'not': 39778,\n",
       " 'also': 39726,\n",
       " 'have': 35792,\n",
       " 'were': 35190,\n",
       " 'has': 33922,\n",
       " 'but': 32114,\n",
       " 'other': 29018,\n",
       " 'their': 28410,\n",
       " 'its': 26355,\n",
       " 'first': 25976,\n",
       " 'they': 25969,\n",
       " 'had': 25522,\n",
       " 'some': 25219,\n",
       " 'more': 23659,\n",
       " 'all': 23598,\n",
       " 'can': 22954,\n",
       " 'most': 22880,\n",
       " 'been': 22853,\n",
       " 'such': 21830,\n",
       " 'who': 21770,\n",
       " 'many': 21703,\n",
       " 'new': 21281,\n",
       " 'there': 20450,\n",
       " 'used': 20253,\n",
       " 'after': 19196,\n",
       " 'american': 18852,\n",
       " 'when': 18698,\n",
       " 'time': 18512,\n",
       " 'into': 18449,\n",
       " 'these': 17796,\n",
       " 'only': 17550,\n",
       " 'see': 17324,\n",
       " 'may': 17101,\n",
       " 'than': 16971,\n",
       " 'i': 16214,\n",
       " 'world': 16105,\n",
       " 'b': 16081,\n",
       " 'd': 15668,\n",
       " 'would': 15514,\n",
       " 'no': 14730,\n",
       " 'however': 14233,\n",
       " 'between': 14089,\n",
       " 'about': 14045,\n",
       " 'over': 13556,\n",
       " 'states': 13525,\n",
       " 'years': 13388,\n",
       " 'war': 13266,\n",
       " 'people': 13217,\n",
       " 'united': 13176,\n",
       " 'during': 13162,\n",
       " 'known': 13035,\n",
       " 'if': 12992,\n",
       " 'called': 12666,\n",
       " 'use': 12550,\n",
       " 'th': 12122,\n",
       " 'often': 11623,\n",
       " 'system': 11621,\n",
       " 'so': 11573,\n",
       " 'history': 11525,\n",
       " 'state': 11446,\n",
       " 'will': 11368,\n",
       " 'up': 11156,\n",
       " 'while': 11156,\n",
       " 'where': 11074,\n",
       " 'english': 10975,\n",
       " 'city': 10846,\n",
       " 'being': 10750,\n",
       " 'then': 10706,\n",
       " 'any': 10667,\n",
       " 'under': 10643,\n",
       " 'out': 10629,\n",
       " 'both': 10562,\n",
       " 'made': 10553,\n",
       " 'e': 10354,\n",
       " 'well': 10351,\n",
       " 'them': 10277,\n",
       " 'government': 10211,\n",
       " 'number': 10185,\n",
       " 'later': 9919,\n",
       " 'him': 9881,\n",
       " 'her': 9800,\n",
       " 'c': 9693,\n",
       " 'since': 9594,\n",
       " 'century': 9556,\n",
       " 'name': 9538,\n",
       " 'part': 9461,\n",
       " 'british': 9372,\n",
       " 'through': 9353,\n",
       " 'because': 9287,\n",
       " 'university': 9205,\n",
       " 'life': 9119,\n",
       " 'early': 9085,\n",
       " 'x': 8981,\n",
       " 'm': 8888,\n",
       " 'like': 8860,\n",
       " 'year': 8824,\n",
       " 'same': 8733,\n",
       " 'became': 8667,\n",
       " 'day': 8630,\n",
       " 'including': 8606,\n",
       " 'each': 8580,\n",
       " 'example': 8570,\n",
       " 'work': 8566,\n",
       " 'even': 8495,\n",
       " 'language': 8472,\n",
       " 'john': 8402,\n",
       " 'although': 8349,\n",
       " 'form': 8231,\n",
       " 'several': 8183,\n",
       " 'national': 8137,\n",
       " 'g': 8019,\n",
       " 'much': 7966,\n",
       " 'very': 7959,\n",
       " 'general': 7925,\n",
       " 'u': 7924,\n",
       " 'before': 7923,\n",
       " 'what': 7712,\n",
       " 'french': 7701,\n",
       " 'against': 7673,\n",
       " 't': 7594,\n",
       " 'links': 7489,\n",
       " 'high': 7477,\n",
       " 'those': 7464,\n",
       " 'n': 7440,\n",
       " 'now': 7404,\n",
       " 'could': 7377,\n",
       " 'second': 7311,\n",
       " 'based': 7265,\n",
       " 'great': 7170,\n",
       " 'another': 7123,\n",
       " 'german': 7114,\n",
       " 'do': 7096,\n",
       " 'de': 7094,\n",
       " 'external': 7084,\n",
       " 'large': 7053,\n",
       " 'modern': 7002,\n",
       " 'list': 6993,\n",
       " 'major': 6983,\n",
       " 'f': 6945,\n",
       " 'different': 6943,\n",
       " 'common': 6926,\n",
       " 'south': 6848,\n",
       " 'series': 6840,\n",
       " 'king': 6804,\n",
       " 'power': 6755,\n",
       " 'set': 6754,\n",
       " 'long': 6750,\n",
       " 'country': 6715,\n",
       " 'she': 6682,\n",
       " 'until': 6654,\n",
       " 'still': 6651,\n",
       " 'law': 6648,\n",
       " 'music': 6630,\n",
       " 'group': 6612,\n",
       " 'north': 6608,\n",
       " 'game': 6604,\n",
       " 'book': 6580,\n",
       " 'film': 6524,\n",
       " 'term': 6503,\n",
       " 'found': 6408,\n",
       " 'we': 6397,\n",
       " 'end': 6394,\n",
       " 'order': 6338,\n",
       " 'political': 6318,\n",
       " 'party': 6295,\n",
       " 'own': 6294,\n",
       " 'church': 6271,\n",
       " 'president': 6268,\n",
       " 'international': 6192,\n",
       " 'usually': 6181,\n",
       " 'death': 6166,\n",
       " 'you': 6083,\n",
       " 'god': 6004,\n",
       " 'ii': 5926,\n",
       " 'area': 5863,\n",
       " 'around': 5856,\n",
       " 'theory': 5809,\n",
       " 'did': 5805,\n",
       " 'include': 5790,\n",
       " 'way': 5788,\n",
       " 'though': 5749,\n",
       " 'small': 5697,\n",
       " 'using': 5691,\n",
       " 'following': 5691,\n",
       " 'left': 5628,\n",
       " 'military': 5616,\n",
       " 'human': 5615,\n",
       " 'point': 5602,\n",
       " 'within': 5594,\n",
       " 'among': 5560,\n",
       " 'p': 5542,\n",
       " 'non': 5534,\n",
       " 'main': 5504,\n",
       " 'population': 5489,\n",
       " 'r': 5435,\n",
       " 'public': 5421,\n",
       " 'considered': 5402,\n",
       " 'due': 5364,\n",
       " 'family': 5362,\n",
       " 'west': 5349,\n",
       " 'computer': 5332,\n",
       " 'east': 5298,\n",
       " 'right': 5296,\n",
       " 'information': 5295,\n",
       " 'popular': 5270,\n",
       " 'important': 5246,\n",
       " 'sometimes': 5197,\n",
       " 'man': 5193,\n",
       " 'old': 5188,\n",
       " 'european': 5171,\n",
       " 'roman': 5146,\n",
       " 'without': 5124,\n",
       " 'last': 5101,\n",
       " 'members': 5089,\n",
       " 'us': 5066,\n",
       " 'given': 5059,\n",
       " 'word': 5037,\n",
       " 'times': 5004,\n",
       " 'h': 4976,\n",
       " 'free': 4905,\n",
       " 'make': 4892,\n",
       " 'age': 4871,\n",
       " 'science': 4842,\n",
       " 'place': 4832,\n",
       " 'house': 4832,\n",
       " 'born': 4814,\n",
       " 'thus': 4784,\n",
       " 'case': 4761,\n",
       " 'l': 4755,\n",
       " 'become': 4735,\n",
       " 'york': 4724,\n",
       " 'does': 4704,\n",
       " 'st': 4697,\n",
       " 'back': 4696,\n",
       " 'others': 4685,\n",
       " 'union': 4684,\n",
       " 'article': 4668,\n",
       " 'led': 4664,\n",
       " 'isbn': 4659,\n",
       " 'should': 4654,\n",
       " 'countries': 4654,\n",
       " 'period': 4649,\n",
       " 'languages': 4645,\n",
       " 'k': 4636,\n",
       " 'line': 4616,\n",
       " 'various': 4612,\n",
       " 'systems': 4604,\n",
       " 'europe': 4591,\n",
       " 'water': 4581,\n",
       " 'central': 4572,\n",
       " 'few': 4562,\n",
       " 'written': 4519,\n",
       " 'began': 4513,\n",
       " 'works': 4505,\n",
       " 'player': 4499,\n",
       " 'home': 4486,\n",
       " 'island': 4483,\n",
       " 'generally': 4470,\n",
       " 'must': 4466,\n",
       " 'air': 4454,\n",
       " 'best': 4444,\n",
       " 'less': 4434,\n",
       " 'original': 4429,\n",
       " 'control': 4427,\n",
       " 'western': 4426,\n",
       " 'school': 4408,\n",
       " 'according': 4386,\n",
       " 'similar': 4379,\n",
       " 'v': 4338,\n",
       " 'england': 4335,\n",
       " 'land': 4304,\n",
       " 'down': 4283,\n",
       " 'force': 4280,\n",
       " 'single': 4271,\n",
       " 'j': 4258,\n",
       " 'how': 4252,\n",
       " 'greek': 4227,\n",
       " 'rather': 4188,\n",
       " 'groups': 4183,\n",
       " 'france': 4163,\n",
       " 'support': 4161,\n",
       " 'published': 4154,\n",
       " 'official': 4153,\n",
       " 'development': 4144,\n",
       " 'space': 4126,\n",
       " 'london': 4117,\n",
       " 'named': 4115,\n",
       " 'data': 4081,\n",
       " 'km': 4074,\n",
       " 'germany': 4073,\n",
       " 'just': 4066,\n",
       " 'said': 4043,\n",
       " 'christian': 4035,\n",
       " 'black': 4030,\n",
       " 'short': 4028,\n",
       " 'empire': 4014,\n",
       " 'games': 4008,\n",
       " 'late': 3998,\n",
       " 'off': 3995,\n",
       " 'every': 3994,\n",
       " 'earth': 3980,\n",
       " 'army': 3975,\n",
       " 'version': 3945,\n",
       " 'college': 3939,\n",
       " 'body': 3925,\n",
       " 'o': 3924,\n",
       " 'company': 3923,\n",
       " 'million': 3919,\n",
       " 'kingdom': 3912,\n",
       " 'economic': 3909,\n",
       " 'social': 3898,\n",
       " 'either': 3867,\n",
       " 'service': 3842,\n",
       " 'along': 3825,\n",
       " 'america': 3804,\n",
       " 'sea': 3804,\n",
       " 'standard': 3794,\n",
       " 'never': 3786,\n",
       " 'court': 3783,\n",
       " 'developed': 3781,\n",
       " 'held': 3776,\n",
       " 'result': 3769,\n",
       " 'field': 3768,\n",
       " 'means': 3765,\n",
       " 'today': 3763,\n",
       " 'light': 3751,\n",
       " 'battle': 3749,\n",
       " 'books': 3738,\n",
       " 'especially': 3735,\n",
       " 'forces': 3732,\n",
       " 'society': 3719,\n",
       " 'further': 3719,\n",
       " 'third': 3713,\n",
       " 'character': 3711,\n",
       " 'team': 3709,\n",
       " 'take': 3698,\n",
       " 'men': 3682,\n",
       " 'republic': 3680,\n",
       " 'took': 3664,\n",
       " 'show': 3662,\n",
       " 'son': 3653,\n",
       " 'possible': 3652,\n",
       " 'fact': 3643,\n",
       " 'good': 3611,\n",
       " 'himself': 3610,\n",
       " 'having': 3604,\n",
       " 'children': 3601,\n",
       " 'present': 3583,\n",
       " 'former': 3572,\n",
       " 'james': 3568,\n",
       " 'natural': 3561,\n",
       " 'process': 3548,\n",
       " 'current': 3530,\n",
       " 'white': 3525,\n",
       " 'jewish': 3520,\n",
       " 'person': 3517,\n",
       " 'words': 3509,\n",
       " 'father': 3498,\n",
       " 'local': 3497,\n",
       " 'open': 3487,\n",
       " 'little': 3485,\n",
       " 'near': 3473,\n",
       " 'days': 3470,\n",
       " 'islands': 3469,\n",
       " 'red': 3448,\n",
       " 'above': 3447,\n",
       " 'soviet': 3443,\n",
       " 'upon': 3431,\n",
       " 'total': 3430,\n",
       " 'side': 3413,\n",
       " 'came': 3408,\n",
       " 'itself': 3404,\n",
       " 'seen': 3399,\n",
       " 'references': 3391,\n",
       " 'january': 3374,\n",
       " 'famous': 3365,\n",
       " 'once': 3354,\n",
       " 'largest': 3349,\n",
       " 'press': 3349,\n",
       " 'full': 3343,\n",
       " 'culture': 3343,\n",
       " 'almost': 3342,\n",
       " 'certain': 3340,\n",
       " 'level': 3340,\n",
       " 'created': 3333,\n",
       " 'river': 3332,\n",
       " 'play': 3316,\n",
       " 'religious': 3298,\n",
       " 'story': 3297,\n",
       " 'role': 3285,\n",
       " 'al': 3279,\n",
       " 'region': 3272,\n",
       " 'member': 3269,\n",
       " 'act': 3265,\n",
       " 'site': 3265,\n",
       " 'image': 3254,\n",
       " 'again': 3249,\n",
       " 'areas': 3236,\n",
       " 'rights': 3225,\n",
       " 'numbers': 3220,\n",
       " 'died': 3219,\n",
       " 'ancient': 3217,\n",
       " 'art': 3214,\n",
       " 'japanese': 3207,\n",
       " 'view': 3195,\n",
       " 'william': 3194,\n",
       " 'movement': 3186,\n",
       " 'community': 3186,\n",
       " 'center': 3184,\n",
       " 'league': 3182,\n",
       " 'canada': 3179,\n",
       " 'minister': 3173,\n",
       " 'played': 3167,\n",
       " 'foreign': 3167,\n",
       " 'actor': 3165,\n",
       " 'related': 3161,\n",
       " 'council': 3158,\n",
       " 'energy': 3156,\n",
       " 'low': 3155,\n",
       " 'real': 3151,\n",
       " 'living': 3143,\n",
       " 'class': 3141,\n",
       " 'december': 3139,\n",
       " 'terms': 3139,\n",
       " 'bc': 3133,\n",
       " 'change': 3132,\n",
       " 'production': 3123,\n",
       " 'type': 3121,\n",
       " 'george': 3119,\n",
       " 'civil': 3115,\n",
       " 'david': 3112,\n",
       " 'source': 3097,\n",
       " 'research': 3088,\n",
       " 'march': 3088,\n",
       " 'head': 3086,\n",
       " 'style': 3084,\n",
       " 'latin': 3084,\n",
       " 'northern': 3082,\n",
       " 'position': 3080,\n",
       " 'july': 3079,\n",
       " 'program': 3075,\n",
       " 'produced': 3073,\n",
       " 'design': 3072,\n",
       " 'chinese': 3070,\n",
       " 'w': 3065,\n",
       " 'software': 3064,\n",
       " 'making': 3063,\n",
       " 'next': 3063,\n",
       " 'charles': 3061,\n",
       " 'particular': 3056,\n",
       " 'women': 3049,\n",
       " 'forms': 3046,\n",
       " 'available': 3044,\n",
       " 'established': 3042,\n",
       " 'television': 3036,\n",
       " 'far': 3033,\n",
       " 'released': 3012,\n",
       " 'special': 3012,\n",
       " 'middle': 3006,\n",
       " 'together': 2980,\n",
       " 'eastern': 2972,\n",
       " 'china': 2967,\n",
       " 'hand': 2967,\n",
       " 'y': 2958,\n",
       " 'emperor': 2958,\n",
       " 'code': 2951,\n",
       " 'instead': 2936,\n",
       " 'might': 2931,\n",
       " 'parts': 2926,\n",
       " 'india': 2923,\n",
       " 'june': 2920,\n",
       " 'band': 2914,\n",
       " 'thought': 2908,\n",
       " 'least': 2894,\n",
       " 'built': 2886,\n",
       " 'prime': 2885,\n",
       " 'function': 2879,\n",
       " 'included': 2875,\n",
       " 'text': 2855,\n",
       " 'study': 2853,\n",
       " 'anti': 2847,\n",
       " 'half': 2847,\n",
       " 'meaning': 2833,\n",
       " 'young': 2826,\n",
       " 'final': 2823,\n",
       " 'always': 2819,\n",
       " 'november': 2809,\n",
       " 'range': 2807,\n",
       " 'trade': 2805,\n",
       " 'capital': 2784,\n",
       " 'per': 2784,\n",
       " 'taken': 2782,\n",
       " 'traditional': 2766,\n",
       " 'august': 2765,\n",
       " 'economy': 2764,\n",
       " 'october': 2762,\n",
       " 'uk': 2759,\n",
       " 'lost': 2758,\n",
       " 'spanish': 2752,\n",
       " 'nature': 2745,\n",
       " 'wrote': 2735,\n",
       " 'april': 2725,\n",
       " 'run': 2717,\n",
       " 'italian': 2716,\n",
       " 'project': 2697,\n",
       " 'live': 2694,\n",
       " 'catholic': 2694,\n",
       " 'uses': 2682,\n",
       " 'addition': 2681,\n",
       " 'september': 2680,\n",
       " 'true': 2674,\n",
       " 'evidence': 2673,\n",
       " 'jews': 2664,\n",
       " 'recent': 2663,\n",
       " 'russian': 2658,\n",
       " 'effect': 2658,\n",
       " 'players': 2655,\n",
       " 'value': 2646,\n",
       " 'historical': 2642,\n",
       " 'won': 2638,\n",
       " 'africa': 2622,\n",
       " 'radio': 2622,\n",
       " 'note': 2621,\n",
       " 'technology': 2618,\n",
       " 'top': 2612,\n",
       " 'rule': 2611,\n",
       " 'model': 2610,\n",
       " 'southern': 2610,\n",
       " 'done': 2610,\n",
       " 'self': 2609,\n",
       " 'whose': 2608,\n",
       " 'australia': 2605,\n",
       " 'influence': 2602,\n",
       " 'themselves': 2598,\n",
       " 'particularly': 2587,\n",
       " 'album': 2575,\n",
       " 'record': 2571,\n",
       " 'rate': 2570,\n",
       " 'irish': 2569,\n",
       " 'philosophy': 2568,\n",
       " 'species': 2568,\n",
       " 'continued': 2563,\n",
       " 'our': 2562,\n",
       " 'subject': 2562,\n",
       " 'star': 2559,\n",
       " 'japan': 2555,\n",
       " 'throughout': 2554,\n",
       " 'love': 2552,\n",
       " 'below': 2546,\n",
       " 'structure': 2546,\n",
       " 'problem': 2542,\n",
       " 'israel': 2539,\n",
       " 'writer': 2535,\n",
       " 'title': 2533,\n",
       " 'referred': 2532,\n",
       " 'cases': 2530,\n",
       " 'author': 2528,\n",
       " 'rock': 2528,\n",
       " 'names': 2526,\n",
       " 'february': 2512,\n",
       " 'therefore': 2509,\n",
       " 'education': 2507,\n",
       " 'whether': 2505,\n",
       " 'paul': 2505,\n",
       " 'come': 2501,\n",
       " 'originally': 2496,\n",
       " 'nations': 2492,\n",
       " 'too': 2490,\n",
       " 'com': 2478,\n",
       " 'action': 2477,\n",
       " 'strong': 2473,\n",
       " 'song': 2470,\n",
       " 'my': 2469,\n",
       " 'town': 2468,\n",
       " 'robert': 2464,\n",
       " 'higher': 2463,\n",
       " 'here': 2450,\n",
       " 'est': 2449,\n",
       " 'cities': 2446,\n",
       " 'elements': 2443,\n",
       " 'independent': 2438,\n",
       " 'eventually': 2438,\n",
       " 'market': 2437,\n",
       " 'network': 2434,\n",
       " 'described': 2428,\n",
       " 'characters': 2426,\n",
       " 'base': 2424,\n",
       " 'female': 2423,\n",
       " 'office': 2423,\n",
       " 'sound': 2419,\n",
       " 'individual': 2412,\n",
       " 'problems': 2411,\n",
       " 'football': 2411,\n",
       " 'despite': 2410,\n",
       " 'events': 2399,\n",
       " 'royal': 2392,\n",
       " 'parliament': 2387,\n",
       " 'formed': 2384,\n",
       " 're': 2384,\n",
       " 'practice': 2381,\n",
       " 'fiction': 2379,\n",
       " 'able': 2378,\n",
       " 'films': 2375,\n",
       " 'la': 2371,\n",
       " 'complex': 2354,\n",
       " 'commonly': 2349,\n",
       " 'african': 2344,\n",
       " 'season': 2340,\n",
       " 'canadian': 2339,\n",
       " 'internet': 2338,\n",
       " 'received': 2335,\n",
       " 'key': 2324,\n",
       " 'lower': 2323,\n",
       " 'leader': 2322,\n",
       " 'aircraft': 2316,\n",
       " 'outside': 2314,\n",
       " 'business': 2302,\n",
       " 'deaths': 2301,\n",
       " 'typically': 2292,\n",
       " 'followed': 2289,\n",
       " 'page': 2288,\n",
       " 'basic': 2286,\n",
       " 'includes': 2285,\n",
       " 'complete': 2282,\n",
       " 'henry': 2282,\n",
       " 'actually': 2279,\n",
       " 'significant': 2276,\n",
       " 'legal': 2274,\n",
       " 'widely': 2273,\n",
       " 'food': 2272,\n",
       " 'male': 2271,\n",
       " 'beginning': 2262,\n",
       " 'elected': 2254,\n",
       " 'fire': 2249,\n",
       " 'news': 2247,\n",
       " 'ireland': 2242,\n",
       " 'births': 2240,\n",
       " 'went': 2236,\n",
       " 'physical': 2235,\n",
       " 'future': 2232,\n",
       " 'ever': 2228,\n",
       " 'cross': 2227,\n",
       " 'laws': 2226,\n",
       " 'scientific': 2226,\n",
       " 'building': 2223,\n",
       " 'types': 2221,\n",
       " 'britain': 2217,\n",
       " 'method': 2213,\n",
       " 'material': 2212,\n",
       " 'cannot': 2211,\n",
       " 'writing': 2200,\n",
       " 'get': 2200,\n",
       " 'independence': 2200,\n",
       " 'services': 2197,\n",
       " 'california': 2193,\n",
       " 'cause': 2192,\n",
       " 'go': 2188,\n",
       " 'believe': 2186,\n",
       " 'close': 2183,\n",
       " 'post': 2179,\n",
       " 'simply': 2174,\n",
       " 'specific': 2168,\n",
       " 'knowledge': 2165,\n",
       " 'return': 2159,\n",
       " 'election': 2155,\n",
       " 'video': 2154,\n",
       " 'points': 2151,\n",
       " 'size': 2151,\n",
       " 'majority': 2147,\n",
       " 'lead': 2146,\n",
       " 'industry': 2144,\n",
       " 'examples': 2143,\n",
       " 'personal': 2142,\n",
       " 'soon': 2141,\n",
       " 'idea': 2141,\n",
       " 'lord': 2141,\n",
       " 'defined': 2136,\n",
       " 'mass': 2135,\n",
       " 'introduced': 2126,\n",
       " 'required': 2123,\n",
       " 'associated': 2112,\n",
       " 'sense': 2112,\n",
       " 'classical': 2106,\n",
       " 'indian': 2099,\n",
       " 'believed': 2099,\n",
       " 'religion': 2098,\n",
       " 'yet': 2094,\n",
       " 'county': 2092,\n",
       " 'co': 2086,\n",
       " 'organization': 2085,\n",
       " 'changes': 2084,\n",
       " 'designed': 2080,\n",
       " 'away': 2077,\n",
       " 'policy': 2077,\n",
       " 'thomas': 2075,\n",
       " 'blue': 2072,\n",
       " 'movie': 2068,\n",
       " 'put': 2063,\n",
       " 'concept': 2062,\n",
       " 'started': 2062,\n",
       " 'find': 2058,\n",
       " 'mother': 2058,\n",
       " 'located': 2056,\n",
       " 'earlier': 2055,\n",
       " 'features': 2048,\n",
       " 'studies': 2048,\n",
       " 'federal': 2045,\n",
       " 'russia': 2044,\n",
       " 'working': 2036,\n",
       " 'sources': 2034,\n",
       " 'currently': 2033,\n",
       " 'brought': 2032,\n",
       " 'online': 2024,\n",
       " 'attack': 2023,\n",
       " 'rules': 2023,\n",
       " 'allowed': 2022,\n",
       " 'stories': 2021,\n",
       " 'added': 2017,\n",
       " 'things': 2015,\n",
       " 'australian': 2015,\n",
       " 'career': 2014,\n",
       " 'across': 2010,\n",
       " 'object': 2008,\n",
       " 'association': 2005,\n",
       " 'founded': 2005,\n",
       " 'provide': 1998,\n",
       " 'limited': 1997,\n",
       " 'greater': 1991,\n",
       " 'mostly': 1990,\n",
       " 'constitution': 1989,\n",
       " 'singer': 1988,\n",
       " 'killed': 1986,\n",
       " 'interest': 1983,\n",
       " 'letters': 1974,\n",
       " 'me': 1974,\n",
       " 'relations': 1973,\n",
       " 'probably': 1971,\n",
       " 'gave': 1970,\n",
       " 'past': 1969,\n",
       " 'letter': 1969,\n",
       " 'simple': 1968,\n",
       " 'your': 1961,\n",
       " 'reference': 1958,\n",
       " 'ball': 1956,\n",
       " 'chief': 1954,\n",
       " 'z': 1949,\n",
       " 'need': 1948,\n",
       " 'effects': 1944,\n",
       " 'growth': 1941,\n",
       " 'remains': 1939,\n",
       " 'success': 1936,\n",
       " 'peace': 1931,\n",
       " 'security': 1929,\n",
       " 'media': 1928,\n",
       " 'novel': 1927,\n",
       " 'park': 1927,\n",
       " 'longer': 1926,\n",
       " 'give': 1923,\n",
       " 'library': 1923,\n",
       " 'spain': 1919,\n",
       " 'say': 1916,\n",
       " 'night': 1916,\n",
       " 'holy': 1913,\n",
       " 'leading': 1911,\n",
       " 'moved': 1909,\n",
       " 'wide': 1909,\n",
       " 'color': 1908,\n",
       " 'remained': 1906,\n",
       " 'wife': 1904,\n",
       " 'dutch': 1900,\n",
       " 'months': 1899,\n",
       " 'iii': 1899,\n",
       " 'better': 1898,\n",
       " 'prize': 1894,\n",
       " 'website': 1893,\n",
       " 'cell': 1892,\n",
       " 'green': 1891,\n",
       " 'speed': 1890,\n",
       " 'becomes': 1890,\n",
       " 'contains': 1890,\n",
       " 'big': 1889,\n",
       " 'dead': 1887,\n",
       " 'already': 1886,\n",
       " 'etc': 1885,\n",
       " 'largely': 1883,\n",
       " 'politics': 1880,\n",
       " 'perhaps': 1879,\n",
       " 'territory': 1878,\n",
       " 'help': 1871,\n",
       " 'larger': 1871,\n",
       " 'appeared': 1865,\n",
       " 'origin': 1864,\n",
       " 'lake': 1863,\n",
       " 'saint': 1863,\n",
       " 'alexander': 1860,\n",
       " 'cultural': 1860,\n",
       " 'asia': 1856,\n",
       " 'claim': 1852,\n",
       " 'companies': 1850,\n",
       " 'actress': 1849,\n",
       " 'italy': 1848,\n",
       " 'definition': 1848,\n",
       " 'whole': 1847,\n",
       " 'lines': 1847,\n",
       " 'parties': 1846,\n",
       " 'refer': 1846,\n",
       " 'matter': 1846,\n",
       " 'attempt': 1844,\n",
       " 'private': 1842,\n",
       " 'saw': 1842,\n",
       " 'makes': 1842,\n",
       " 'era': 1840,\n",
       " 'richard': 1840,\n",
       " 'directly': 1839,\n",
       " 'caused': 1834,\n",
       " 'separate': 1831,\n",
       " 'authority': 1830,\n",
       " 'coast': 1829,\n",
       " 'turn': 1828,\n",
       " 'successful': 1824,\n",
       " 'surface': 1824,\n",
       " 'literature': 1823,\n",
       " 'highly': 1821,\n",
       " 'health': 1820,\n",
       " 'results': 1819,\n",
       " 'married': 1818,\n",
       " 'double': 1818,\n",
       " 'edition': 1817,\n",
       " 'towards': 1815,\n",
       " 'minor': 1812,\n",
       " 'revolution': 1811,\n",
       " 'products': 1810,\n",
       " 'produce': 1807,\n",
       " 'oil': 1804,\n",
       " 'date': 1803,\n",
       " 'division': 1799,\n",
       " 'primary': 1799,\n",
       " 'whom': 1798,\n",
       " 'treaty': 1795,\n",
       " 'status': 1793,\n",
       " 'programming': 1790,\n",
       " 'direct': 1787,\n",
       " 'nation': 1786,\n",
       " 'latter': 1785,\n",
       " 'playing': 1781,\n",
       " 'native': 1778,\n",
       " 'basis': 1775,\n",
       " 'analysis': 1774,\n",
       " 'enough': 1771,\n",
       " 'issues': 1767,\n",
       " 'sun': 1766,\n",
       " 'queen': 1765,\n",
       " 'web': 1764,\n",
       " 'course': 1764,\n",
       " 'rome': 1763,\n",
       " 'likely': 1763,\n",
       " 'exist': 1759,\n",
       " 'peter': 1755,\n",
       " 'release': 1755,\n",
       " 'allow': 1749,\n",
       " 'reason': 1748,\n",
       " 'blood': 1748,\n",
       " 'museum': 1746,\n",
       " 'machine': 1745,\n",
       " 'commercial': 1744,\n",
       " 'provided': 1744,\n",
       " 'amount': 1743,\n",
       " 'washington': 1740,\n",
       " 'gas': 1737,\n",
       " 'chemical': 1735,\n",
       " 'money': 1733,\n",
       " 'jesus': 1728,\n",
       " 'smaller': 1727,\n",
       " 'nearly': 1725,\n",
       " 'digital': 1725,\n",
       " 'congress': 1724,\n",
       " 'memory': 1724,\n",
       " 'replaced': 1722,\n",
       " 'length': 1722,\n",
       " 'functions': 1722,\n",
       " 'claims': 1721,\n",
       " 'tradition': 1721,\n",
       " 'divided': 1718,\n",
       " 'average': 1715,\n",
       " 'tv': 1710,\n",
       " 'metal': 1707,\n",
       " 'degree': 1706,\n",
       " 'director': 1703,\n",
       " 'difficult': 1701,\n",
       " 'served': 1700,\n",
       " 'read': 1699,\n",
       " 'baseball': 1698,\n",
       " 'collection': 1697,\n",
       " 'property': 1696,\n",
       " 'ten': 1695,\n",
       " 'christ': 1695,\n",
       " 'notable': 1694,\n",
       " 'claimed': 1694,\n",
       " 'democratic': 1692,\n",
       " 'recently': 1692,\n",
       " 'access': 1691,\n",
       " 'mid': 1690,\n",
       " 'michael': 1689,\n",
       " 'bbc': 1688,\n",
       " 'finally': 1686,\n",
       " 'changed': 1685,\n",
       " 'ground': 1685,\n",
       " 'elections': 1683,\n",
       " 'sent': 1683,\n",
       " 'front': 1682,\n",
       " 'medical': 1681,\n",
       " 'highest': 1680,\n",
       " 'involved': 1677,\n",
       " 'mark': 1674,\n",
       " 'records': 1674,\n",
       " 'liberal': 1672,\n",
       " 'ideas': 1671,\n",
       " 'performance': 1669,\n",
       " 'bwv': 1668,\n",
       " 'variety': 1664,\n",
       " 'entire': 1663,\n",
       " 'schools': 1662,\n",
       " 'frequently': 1662,\n",
       " 'animals': 1660,\n",
       " 'programs': 1659,\n",
       " 'don': 1658,\n",
       " 'hard': 1658,\n",
       " 'section': 1656,\n",
       " 'club': 1655,\n",
       " 'methods': 1650,\n",
       " 'returned': 1650,\n",
       " 'louis': 1649,\n",
       " 'discovered': 1640,\n",
       " 'relatively': 1637,\n",
       " 'call': 1635,\n",
       " 'board': 1635,\n",
       " 'increased': 1629,\n",
       " 'unit': 1628,\n",
       " 'students': 1625,\n",
       " 'relationship': 1624,\n",
       " 'orthodox': 1621,\n",
       " 'objects': 1620,\n",
       " 'bill': 1618,\n",
       " 'nuclear': 1617,\n",
       " 'smith': 1617,\n",
       " 'adopted': 1616,\n",
       " 'fall': 1615,\n",
       " 'appear': 1614,\n",
       " 'assembly': 1612,\n",
       " 'san': 1611,\n",
       " 'rest': 1610,\n",
       " 'appears': 1610,\n",
       " 'know': 1609,\n",
       " 'conditions': 1605,\n",
       " 'hall': 1604,\n",
       " 'bank': 1602,\n",
       " 'map': 1602,\n",
       " 'increase': 1601,\n",
       " 'element': 1600,\n",
       " 'existence': 1599,\n",
       " 'user': 1599,\n",
       " 'pope': 1598,\n",
       " 'award': 1598,\n",
       " 'engine': 1598,\n",
       " ...}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index to word and word to index mapping\n",
    "# the order of embedding is the same as word_to_idx. Like embedding[0] is the embedding for word[0]\n",
    "# these two sets will also be used in the evaluation\n",
    "idx_to_word = list(vocab.keys())\n",
    "word_to_idx = {word: i for i, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 0),\n",
       " ('of', 1),\n",
       " ('and', 2),\n",
       " ('one', 3),\n",
       " ('in', 4),\n",
       " ('a', 5),\n",
       " ('to', 6),\n",
       " ('zero', 7),\n",
       " ('nine', 8),\n",
       " ('two', 9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # store word frequency that will be used in negative sampling\n",
    "word_counts = np.array(list(vocab.values()), dtype = np.float32)\n",
    "word_freqs = word_counts / np.sum(word_counts)\n",
    "word_freqs = word_freqs ** (3./4.)\n",
    "word_freqs = word_freqs / np.sum(word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6231162e-02, 1.0509998e-02, 8.0359895e-03, ..., 5.0128656e-06,\n",
       "       5.0128656e-06, 1.1670408e-02], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(idx_to_word)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement data loader\n",
    "class WordEmbeddingDataLoader(tud.Dataset):\n",
    "    def __init__(self, text, idx_to_word, word_to_idx, word_counts, word_freqs):\n",
    "        '''\n",
    "        text: a list of words\n",
    "        '''\n",
    "        super(WordEmbeddingDataLoader, self).__init__()\n",
    "        self.text_encoded = [word_to_idx.get(t, word_to_idx[\"<unk>\"]) for t in text]\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
    "        \n",
    "        self.idx_to_word = idx_to_word\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.word_counts = torch.Tensor(word_counts)\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        '''\n",
    "        return len of the dataset\n",
    "        '''\n",
    "        return len(self.text_encoded)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        return (centor_word, positive words around centor word, K negative samples for each positive word)\n",
    "        '''\n",
    "        \n",
    "        center_word = self.text_encoded[idx]\n",
    "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] # avoid index out of bound\n",
    "        pos_words = self.text_encoded[pos_indices]\n",
    "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True) # for each pos_word, select K negative words based on word_freqs\n",
    "        \n",
    "        return center_word, pos_words, neg_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WordEmbeddingDataLoader(text, idx_to_word, word_to_idx, word_counts, word_freqs)\n",
    "dataLoader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.centor_word_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False) # (vocab_size, embed_size)\n",
    "        self.context_word_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False) # (vocab_size, embed_size)\n",
    "        \n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\n",
    "        '''\n",
    "        input_labels: (batch_size,)\n",
    "        pos_labels: (batch_size, 2 * C)\n",
    "        neg_labels: (batch_size, (2 * C * K))\n",
    "        '''\n",
    "        \n",
    "        # implement Equation (4) in https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n",
    "        batch_size = input_labels.shape[0]\n",
    "        \n",
    "        input_embedding = self.centor_word_embed(input_labels) # (batch_size, embed_size)\n",
    "        pos_embedding = self.context_word_embed(pos_labels) # (batch_size, 2C, embed_size)\n",
    "        neg_embedding = self.context_word_embed(neg_labels) # (batch_size, 2CK, embed_size)\n",
    "        \n",
    "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # (batch_size, 2C). The shape of input_embedding.unsqueeze(2) is (batch_size, embed_size, 1)\n",
    "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # (batch_size, 2CK)\n",
    "        \n",
    "        log_pos = F.logsigmoid(log_pos).sum(axis=1) # (batch_size,)\n",
    "        log_neg = F.logsigmoid(log_neg).sum(axis=1) # (batch_size,)\n",
    "        \n",
    "        loss = log_pos + log_neg\n",
    "        return -loss # since Equation (4) in the paper is the objective. We maximize the objective but minimize loss\n",
    "    \n",
    "    def input_embeddings(self):\n",
    "        # use center word embedding as the final word embedding\n",
    "        return self.centor_word_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss: 2466.075927734375\n",
      "epoch: 0, batch: 1000, loss: 562.1318969726562\n",
      "epoch: 0, batch: 2000, loss: 408.2147216796875\n",
      "epoch: 0, batch: 3000, loss: 280.123046875\n",
      "epoch: 0, batch: 4000, loss: 195.11968994140625\n",
      "epoch: 0, batch: 5000, loss: 216.25302124023438\n",
      "epoch: 0, batch: 6000, loss: 143.4933319091797\n",
      "epoch: 0, batch: 7000, loss: 116.78927612304688\n",
      "epoch: 0, batch: 8000, loss: 126.276611328125\n",
      "epoch: 0, batch: 9000, loss: 144.72784423828125\n",
      "epoch: 0, batch: 10000, loss: 96.56869506835938\n",
      "epoch: 0, batch: 11000, loss: 87.42467498779297\n",
      "epoch: 0, batch: 12000, loss: 103.14344787597656\n",
      "epoch: 0, batch: 13000, loss: 96.37692260742188\n",
      "epoch: 0, batch: 14000, loss: 89.42659759521484\n",
      "epoch: 0, batch: 15000, loss: 69.29397583007812\n",
      "epoch: 0, batch: 16000, loss: 64.8114013671875\n",
      "epoch: 0, batch: 17000, loss: 68.66221618652344\n",
      "epoch: 0, batch: 18000, loss: 82.23294067382812\n",
      "epoch: 0, batch: 19000, loss: 54.06744384765625\n",
      "epoch: 0, batch: 20000, loss: 70.83740234375\n",
      "epoch: 0, batch: 21000, loss: 54.810237884521484\n",
      "epoch: 0, batch: 22000, loss: 51.994651794433594\n",
      "epoch: 0, batch: 23000, loss: 56.87190246582031\n",
      "epoch: 0, batch: 24000, loss: 53.31330871582031\n",
      "epoch: 0, batch: 25000, loss: 47.443077087402344\n",
      "epoch: 0, batch: 26000, loss: 53.092926025390625\n",
      "epoch: 0, batch: 27000, loss: 51.38838195800781\n",
      "epoch: 0, batch: 28000, loss: 48.38013458251953\n",
      "epoch: 0, batch: 29000, loss: 43.727821350097656\n",
      "epoch: 0, batch: 30000, loss: 53.725196838378906\n",
      "epoch: 0, batch: 31000, loss: 49.75012969970703\n",
      "epoch: 0, batch: 32000, loss: 50.825927734375\n",
      "epoch: 0, batch: 33000, loss: 42.29494857788086\n",
      "epoch: 0, batch: 34000, loss: 43.56721496582031\n",
      "epoch: 0, batch: 35000, loss: 43.13948059082031\n",
      "epoch: 0, batch: 36000, loss: 41.97203826904297\n",
      "epoch: 0, batch: 37000, loss: 45.07292938232422\n",
      "epoch: 0, batch: 38000, loss: 42.491085052490234\n",
      "epoch: 0, batch: 39000, loss: 46.68785858154297\n",
      "epoch: 0, batch: 40000, loss: 44.781761169433594\n",
      "epoch: 0, batch: 41000, loss: 43.136924743652344\n",
      "epoch: 0, batch: 42000, loss: 44.00106430053711\n",
      "epoch: 0, batch: 43000, loss: 40.96917724609375\n",
      "epoch: 0, batch: 44000, loss: 46.94386291503906\n",
      "epoch: 0, batch: 45000, loss: 40.483978271484375\n",
      "epoch: 0, batch: 46000, loss: 42.231040954589844\n",
      "epoch: 0, batch: 47000, loss: 41.8361930847168\n",
      "epoch: 0, batch: 48000, loss: 39.38572311401367\n",
      "epoch: 0, batch: 49000, loss: 40.18342590332031\n",
      "epoch: 0, batch: 50000, loss: 41.53038024902344\n",
      "epoch: 0, batch: 51000, loss: 44.539031982421875\n",
      "epoch: 0, batch: 52000, loss: 38.56099319458008\n",
      "epoch: 0, batch: 53000, loss: 42.4983024597168\n",
      "epoch: 0, batch: 54000, loss: 38.16940689086914\n",
      "epoch: 0, batch: 55000, loss: 39.703521728515625\n",
      "epoch: 0, batch: 56000, loss: 38.472801208496094\n",
      "epoch: 0, batch: 57000, loss: 38.653411865234375\n",
      "epoch: 0, batch: 58000, loss: 38.21653747558594\n",
      "epoch: 0, batch: 59000, loss: 38.14936828613281\n",
      "epoch: 0, batch: 60000, loss: 39.3534049987793\n",
      "epoch: 0, batch: 61000, loss: 39.6802978515625\n",
      "epoch: 0, batch: 62000, loss: 40.685081481933594\n",
      "epoch: 0, batch: 63000, loss: 40.826759338378906\n",
      "epoch: 0, batch: 64000, loss: 39.1984977722168\n",
      "epoch: 0, batch: 65000, loss: 37.85343933105469\n",
      "epoch: 0, batch: 66000, loss: 36.3021240234375\n",
      "epoch: 0, batch: 67000, loss: 36.377586364746094\n",
      "epoch: 0, batch: 68000, loss: 38.81293869018555\n",
      "epoch: 0, batch: 69000, loss: 37.2871208190918\n",
      "epoch: 0, batch: 70000, loss: 37.79283905029297\n",
      "epoch: 0, batch: 71000, loss: 37.98100280761719\n",
      "epoch: 0, batch: 72000, loss: 37.59992980957031\n",
      "epoch: 0, batch: 73000, loss: 38.74529266357422\n",
      "epoch: 0, batch: 74000, loss: 37.55610656738281\n",
      "epoch: 0, batch: 75000, loss: 37.099853515625\n",
      "epoch: 0, batch: 76000, loss: 38.70706558227539\n",
      "epoch: 0, batch: 77000, loss: 35.8026008605957\n",
      "epoch: 0, batch: 78000, loss: 36.76610565185547\n",
      "epoch: 0, batch: 79000, loss: 36.72492218017578\n",
      "epoch: 0, batch: 80000, loss: 36.58046340942383\n",
      "epoch: 0, batch: 81000, loss: 37.6331787109375\n",
      "epoch: 0, batch: 82000, loss: 36.582977294921875\n",
      "epoch: 0, batch: 83000, loss: 37.67259979248047\n",
      "epoch: 0, batch: 84000, loss: 36.80692672729492\n",
      "epoch: 0, batch: 85000, loss: 37.856170654296875\n",
      "epoch: 0, batch: 86000, loss: 34.8258171081543\n",
      "epoch: 0, batch: 87000, loss: 38.95021057128906\n",
      "epoch: 0, batch: 88000, loss: 38.63077163696289\n",
      "epoch: 0, batch: 89000, loss: 37.395164489746094\n",
      "epoch: 0, batch: 90000, loss: 35.845699310302734\n",
      "epoch: 0, batch: 91000, loss: 37.283058166503906\n",
      "epoch: 0, batch: 92000, loss: 36.630157470703125\n",
      "epoch: 0, batch: 93000, loss: 37.24055480957031\n",
      "epoch: 0, batch: 94000, loss: 35.53184127807617\n",
      "epoch: 0, batch: 95000, loss: 38.007171630859375\n",
      "epoch: 0, batch: 96000, loss: 35.786861419677734\n",
      "epoch: 0, batch: 97000, loss: 34.25556945800781\n",
      "epoch: 0, batch: 98000, loss: 36.58647537231445\n",
      "epoch: 0, batch: 99000, loss: 38.25462341308594\n",
      "epoch: 0, batch: 100000, loss: 35.6497917175293\n",
      "epoch: 0, batch: 101000, loss: 35.99232864379883\n",
      "epoch: 0, batch: 102000, loss: 34.5037841796875\n",
      "epoch: 0, batch: 103000, loss: 33.95780944824219\n",
      "epoch: 0, batch: 104000, loss: 37.73053741455078\n",
      "epoch: 0, batch: 105000, loss: 35.03478240966797\n",
      "epoch: 0, batch: 106000, loss: 34.93122100830078\n",
      "epoch: 0, batch: 107000, loss: 37.66149139404297\n",
      "epoch: 0, batch: 108000, loss: 36.673946380615234\n",
      "epoch: 0, batch: 109000, loss: 35.41780090332031\n",
      "epoch: 0, batch: 110000, loss: 37.78717041015625\n",
      "epoch: 0, batch: 111000, loss: 35.43149948120117\n",
      "epoch: 0, batch: 112000, loss: 34.20154571533203\n",
      "epoch: 0, batch: 113000, loss: 38.99000549316406\n",
      "epoch: 0, batch: 114000, loss: 36.968170166015625\n",
      "epoch: 0, batch: 115000, loss: 35.26683044433594\n",
      "epoch: 0, batch: 116000, loss: 34.55632400512695\n",
      "epoch: 0, batch: 117000, loss: 37.57416534423828\n",
      "epoch: 0, batch: 118000, loss: 36.08250427246094\n",
      "epoch: 0, batch: 119000, loss: 35.40726852416992\n",
      "epoch: 1, batch: 0, loss: 35.310020446777344\n",
      "epoch: 1, batch: 1000, loss: 35.802093505859375\n",
      "epoch: 1, batch: 2000, loss: 35.992671966552734\n",
      "epoch: 1, batch: 3000, loss: 35.88154602050781\n",
      "epoch: 1, batch: 4000, loss: 35.14612579345703\n",
      "epoch: 1, batch: 5000, loss: 35.19947052001953\n",
      "epoch: 1, batch: 6000, loss: 36.536746978759766\n",
      "epoch: 1, batch: 7000, loss: 35.00756072998047\n",
      "epoch: 1, batch: 8000, loss: 35.91600799560547\n",
      "epoch: 1, batch: 9000, loss: 34.136905670166016\n",
      "epoch: 1, batch: 10000, loss: 34.758914947509766\n",
      "epoch: 1, batch: 11000, loss: 35.227447509765625\n",
      "epoch: 1, batch: 12000, loss: 35.42942810058594\n",
      "epoch: 1, batch: 13000, loss: 35.423458099365234\n",
      "epoch: 1, batch: 14000, loss: 35.25221633911133\n",
      "epoch: 1, batch: 15000, loss: 34.13239288330078\n",
      "epoch: 1, batch: 16000, loss: 35.62466049194336\n",
      "epoch: 1, batch: 17000, loss: 36.15114212036133\n",
      "epoch: 1, batch: 18000, loss: 34.65931701660156\n",
      "epoch: 1, batch: 19000, loss: 34.507537841796875\n",
      "epoch: 1, batch: 20000, loss: 35.763885498046875\n",
      "epoch: 1, batch: 21000, loss: 33.059261322021484\n",
      "epoch: 1, batch: 22000, loss: 34.89992141723633\n",
      "epoch: 1, batch: 23000, loss: 34.442771911621094\n",
      "epoch: 1, batch: 24000, loss: 34.68345260620117\n",
      "epoch: 1, batch: 25000, loss: 37.130821228027344\n",
      "epoch: 1, batch: 26000, loss: 36.08832550048828\n",
      "epoch: 1, batch: 27000, loss: 34.59730529785156\n",
      "epoch: 1, batch: 28000, loss: 33.59270095825195\n",
      "epoch: 1, batch: 29000, loss: 37.52484893798828\n",
      "epoch: 1, batch: 30000, loss: 34.764747619628906\n",
      "epoch: 1, batch: 31000, loss: 32.83133316040039\n",
      "epoch: 1, batch: 32000, loss: 34.31672668457031\n",
      "epoch: 1, batch: 33000, loss: 34.7341194152832\n",
      "epoch: 1, batch: 34000, loss: 34.26656723022461\n",
      "epoch: 1, batch: 35000, loss: 34.70124053955078\n",
      "epoch: 1, batch: 36000, loss: 34.11603927612305\n",
      "epoch: 1, batch: 37000, loss: 33.427833557128906\n",
      "epoch: 1, batch: 38000, loss: 36.1768798828125\n",
      "epoch: 1, batch: 39000, loss: 35.335716247558594\n",
      "epoch: 1, batch: 40000, loss: 35.62925720214844\n",
      "epoch: 1, batch: 41000, loss: 33.1612434387207\n",
      "epoch: 1, batch: 42000, loss: 34.11709213256836\n",
      "epoch: 1, batch: 43000, loss: 35.42338562011719\n",
      "epoch: 1, batch: 44000, loss: 34.780479431152344\n",
      "epoch: 1, batch: 45000, loss: 33.29780197143555\n",
      "epoch: 1, batch: 46000, loss: 35.736228942871094\n",
      "epoch: 1, batch: 47000, loss: 35.30345153808594\n",
      "epoch: 1, batch: 48000, loss: 35.0389404296875\n",
      "epoch: 1, batch: 49000, loss: 35.310646057128906\n",
      "epoch: 1, batch: 50000, loss: 34.329612731933594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch: 51000, loss: 34.474151611328125\n",
      "epoch: 1, batch: 52000, loss: 35.43360900878906\n",
      "epoch: 1, batch: 53000, loss: 34.22370147705078\n",
      "epoch: 1, batch: 54000, loss: 34.114776611328125\n",
      "epoch: 1, batch: 55000, loss: 34.41321563720703\n",
      "epoch: 1, batch: 56000, loss: 33.7686882019043\n",
      "epoch: 1, batch: 57000, loss: 37.052940368652344\n",
      "epoch: 1, batch: 58000, loss: 36.223026275634766\n",
      "epoch: 1, batch: 59000, loss: 34.479522705078125\n",
      "epoch: 1, batch: 60000, loss: 34.014495849609375\n",
      "epoch: 1, batch: 61000, loss: 34.220611572265625\n",
      "epoch: 1, batch: 62000, loss: 33.95252227783203\n",
      "epoch: 1, batch: 63000, loss: 34.91365051269531\n",
      "epoch: 1, batch: 64000, loss: 33.761741638183594\n",
      "epoch: 1, batch: 65000, loss: 35.484825134277344\n",
      "epoch: 1, batch: 66000, loss: 34.574745178222656\n",
      "epoch: 1, batch: 67000, loss: 34.67189025878906\n",
      "epoch: 1, batch: 68000, loss: 34.89488220214844\n",
      "epoch: 1, batch: 69000, loss: 34.25187683105469\n",
      "epoch: 1, batch: 70000, loss: 33.44493103027344\n",
      "epoch: 1, batch: 71000, loss: 34.85498046875\n",
      "epoch: 1, batch: 72000, loss: 33.74031066894531\n",
      "epoch: 1, batch: 73000, loss: 34.085792541503906\n",
      "epoch: 1, batch: 74000, loss: 35.282310485839844\n",
      "epoch: 1, batch: 75000, loss: 33.43849182128906\n",
      "epoch: 1, batch: 76000, loss: 32.617889404296875\n",
      "epoch: 1, batch: 77000, loss: 34.52318572998047\n",
      "epoch: 1, batch: 78000, loss: 34.56592559814453\n",
      "epoch: 1, batch: 79000, loss: 34.56416320800781\n",
      "epoch: 1, batch: 80000, loss: 34.213592529296875\n",
      "epoch: 1, batch: 81000, loss: 34.48087692260742\n",
      "epoch: 1, batch: 82000, loss: 33.46763610839844\n",
      "epoch: 1, batch: 83000, loss: 34.93562316894531\n",
      "epoch: 1, batch: 84000, loss: 35.32118225097656\n",
      "epoch: 1, batch: 85000, loss: 33.90019989013672\n",
      "epoch: 1, batch: 86000, loss: 33.749847412109375\n",
      "epoch: 1, batch: 87000, loss: 33.466007232666016\n",
      "epoch: 1, batch: 88000, loss: 35.3717041015625\n",
      "epoch: 1, batch: 89000, loss: 34.46475601196289\n",
      "epoch: 1, batch: 90000, loss: 33.4245719909668\n",
      "epoch: 1, batch: 91000, loss: 33.814247131347656\n",
      "epoch: 1, batch: 92000, loss: 33.427490234375\n",
      "epoch: 1, batch: 93000, loss: 32.80613327026367\n",
      "epoch: 1, batch: 94000, loss: 33.67481994628906\n",
      "epoch: 1, batch: 95000, loss: 34.33870315551758\n",
      "epoch: 1, batch: 96000, loss: 34.43846893310547\n",
      "epoch: 1, batch: 97000, loss: 34.69318389892578\n",
      "epoch: 1, batch: 98000, loss: 33.571372985839844\n",
      "epoch: 1, batch: 99000, loss: 34.96678924560547\n",
      "epoch: 1, batch: 100000, loss: 33.48402404785156\n",
      "epoch: 1, batch: 101000, loss: 33.83191680908203\n",
      "epoch: 1, batch: 102000, loss: 33.150123596191406\n",
      "epoch: 1, batch: 103000, loss: 34.185997009277344\n",
      "epoch: 1, batch: 104000, loss: 32.889183044433594\n",
      "epoch: 1, batch: 105000, loss: 33.485191345214844\n",
      "epoch: 1, batch: 106000, loss: 33.65839767456055\n",
      "epoch: 1, batch: 107000, loss: 34.327632904052734\n",
      "epoch: 1, batch: 108000, loss: 33.46786880493164\n",
      "epoch: 1, batch: 109000, loss: 32.193626403808594\n",
      "epoch: 1, batch: 110000, loss: 33.96128845214844\n",
      "epoch: 1, batch: 111000, loss: 33.12165832519531\n",
      "epoch: 1, batch: 112000, loss: 33.01662826538086\n",
      "epoch: 1, batch: 113000, loss: 31.833805084228516\n",
      "epoch: 1, batch: 114000, loss: 32.34502410888672\n",
      "epoch: 1, batch: 115000, loss: 33.47856903076172\n",
      "epoch: 1, batch: 116000, loss: 33.40636444091797\n",
      "epoch: 1, batch: 117000, loss: 34.93946838378906\n",
      "epoch: 1, batch: 118000, loss: 32.892120361328125\n",
      "epoch: 1, batch: 119000, loss: 33.3558349609375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (center_word, pos_words, neg_words) in enumerate(dataLoader):\n",
    "        \n",
    "        center_word = center_word.long()\n",
    "        pos_words = pos_words.long()\n",
    "        neg_words = neg_words.long()\n",
    "        if USE_CUDA:\n",
    "            center_word = center_word.cuda()\n",
    "            pos_words = pos_words.cuda()\n",
    "            neg_words = neg_words.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = model(center_word, pos_words, neg_words).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch: {}, batch: {}, loss: {}\".format(epoch, i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simplex-999.txt contains word1, word2, similarity by human\n",
    "# the idea is to compute the spearman correlation between predicted similarity and human similarity\n",
    "# the better the correlation, the better of our embedding\n",
    "def evaluate(filename, embedding_weights, word_to_idx): \n",
    "    data = pd.read_csv(filename, sep=\"\\t\")\n",
    "    human_similarity = []\n",
    "    model_similarity = []\n",
    "    for i in data.iloc[:, 0:2].index:\n",
    "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
    "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
    "            continue\n",
    "        else:\n",
    "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
    "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
    "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
    "            human_similarity.append(float(data.iloc[i, 2]))\n",
    "\n",
    "    # (correlation, p-value). 1 - p-value sorts of represent the confidence of the computed correlation\n",
    "    return scipy.stats.spearmanr(human_similarity, model_similarity)\n",
    "\n",
    "def find_nearest(word, word_to_idx, idx_to_word, embedding_weights):\n",
    "    index = word_to_idx[word]\n",
    "    embedding = embedding_weights[index]\n",
    "    # loop through all the embedding and compute cosine distance between each embedding and the embedding of given word\n",
    "    cosine_distance = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
    "    # select top 10 words with smallest distance to the given word\n",
    "    return [idx_to_word[i] for i in cosine_distance.argsort()[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.input_embeddings()\n",
    "# np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
    "# torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simlex-999 SpearmanrResult(correlation=-0.053366410819609765, pvalue=0.09948912566926381)\n"
     ]
    }
   ],
   "source": [
    "print(\"simlex-999\", evaluate(\"simlex-999.txt\", embedding_weights, word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good ['good', 'either', 'even', 'given', 'means', 'because', 'without', 'usually', 'therefore', 'them']\n",
      "fresh ['fresh', 'shady', 'predicting', 'folk', 'recall', 'cores', 'aclu', 'associated', 'abused', 'fin']\n",
      "monster ['monster', 'preparedness', 'infections', 'routledge', 'paint', 'trio', 'argument', 'geology', 'crows', 'presented']\n",
      "green ['green', 'black', 'group', 'red', 'and', 'white', 'etc', 'through', 'with', 'from']\n",
      "like ['like', 'or', 'called', 'also', 'thus', 'being', 'with', 'include', 'using', 'instead']\n",
      "america ['america', 'north', 'europe', 'south', 'central', 'east', 'africa', 'southern', 'west', 'western']\n",
      "chicago ['chicago', 'university', 'press', 'state', 'japan', 'history', 'city', 'studies', 'education', 'uk']\n",
      "work ['work', 'being', 'created', 'while', 'their', 'having', 'was', 'made', 'were', 'which']\n",
      "computer ['computer', 'information', 'systems', 'software', 'based', 'technology', 'uses', 'system', 'using', 'source']\n",
      "language ['language', 'modern', 'languages', 'culture', 'include', 'popular', 'word', 'forms', 'non', 'latin']\n"
     ]
    }
   ],
   "source": [
    "for word in [\"good\", \"fresh\", \"monster\", \"green\", \"like\", \"america\", \"chicago\", \"work\", \"computer\", \"language\"]:\n",
    "    print(word, find_nearest(word, word_to_idx, idx_to_word, embedding_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

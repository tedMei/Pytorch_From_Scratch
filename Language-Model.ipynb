{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext # https://github.com/pytorch/text, used to prepare language model dataset\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "# add seed to make sure the result can be reproduced\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_SIZE = 100\n",
    "HIDDEN_SIZE = 100\n",
    "NLAYERS = 2\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(lower = True) # lowercase the text, https://pytorch.org/text/data.html#fields\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=\"./text\", \n",
    "    train=\"text.train.txt\", validation=\"text.dev.txt\", test=\"text.test.txt\", text_field=TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.datasets.language_modeling.LanguageModelingDataset"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary using torchtextï¼Œ max_size restricts the vocabulary size (order by frequency)\n",
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50002"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', 'the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.itos[:10] # torchtext adds two more word <unk> and <pad> into the vocabulary, itos is index to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=device, bptt_len=64, repeat=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine a training batch\n",
    "it = iter(train_iter)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.text]:[torch.cuda.LongTensor of size 64x32 (GPU 0)]\n",
       "\t[.target]:[torch.cuda.LongTensor of size 64x32 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch # batch.text and batch.target is of shape [seq_len, batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of splitting an atom which as we know today is the source of atomic\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1].data])) # the first batch of batch.text has 64 words (seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in pairs and then group into trios of pairs which are the smallest visible units of matter this parallels with the structure of modern atomic theory in which pairs or triplets of supposedly fundamental quarks combine to create most typical forms of matter they had also suggested the possibility of splitting an atom which as we know today is the source of atomic energy\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1].data])) # the first batch of batch.traget is just one word offset of batch.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, nlayers, bidirectional=True, dropout=0.3):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, \n",
    "                            num_layers=nlayers, \n",
    "                            bidirectional=bidirectional, \n",
    "                            dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.nlayers = nlayers\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, text, hidden):\n",
    "        '''\n",
    "        text: seq_length * batch_size\n",
    "        '''\n",
    "        embed = self.dropout(self.embed(text)) # (seq_length, batch_size, embed_size)\n",
    "        output, hidden = self.lstm(embed, hidden) # output: (seq_length, batch, num_directions * hidden_size), hidden: (nlayers*num_directions, batch, hidden_size), cell: (nlayers*num_directions, batch, hidden_size)\n",
    "        # use output here because we want to know the prediction at every time t rather than only the last stage\n",
    "        out_vocab = self.fc(output.view(-1, output.shape[2])) # (seq_len * batch_size, vocab_size)\n",
    "        out_vocab = out_vocab.view(output.shape[0], output.shape[1], -1) # (seq_len, batch_size, vocab_size)\n",
    "\n",
    "        return out_vocab, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, requires_grad = True):\n",
    "        # initialize the first hidden and cell state of lstm\n",
    "        weight = next(self.parameters()) # smart way to avoid checking if the model weight is in gpu or not\n",
    "        return (weight.new_zeros((self.nlayers * 2, batch_size, self.hidden_size), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((self.nlayers * 2, batch_size, self.hidden_size), requires_grad=requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(vocab_size = len(TEXT.vocab), \n",
    "                 embed_size = EMBEDDING_SIZE, \n",
    "                 hidden_size = HIDDEN_SIZE, \n",
    "                 nlayers = NLAYERS, \n",
    "                 bidirectional=True, \n",
    "                 dropout=0.3)\n",
    "if USE_CUDA:\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embed): Embedding(50002, 100)\n",
       "  (lstm): LSTM(100, 100, num_layers=2, dropout=0.3, bidirectional=True)\n",
       "  (fc): Linear(in_features=200, out_features=50002, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach hidden state so that BPTT will not work on all the words. Instead, set a new starting position in each batch\n",
    "def repackage_hidden(hidden):\n",
    "    if isinstance(hidden, torch.Tensor):\n",
    "        return hidden.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in hidden)\n",
    "    \n",
    "def evaluate(model, val_iter):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    it = iter(val_iter)\n",
    "    total_count = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "    \n",
    "        for i, batch in enumerate(it):\n",
    "            data, target = batch.text, batch.target # data: (seq_length, batch_size)\n",
    "            if USE_CUDA:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output, hidden = model(data, hidden) # output: (seq_len, batch_size, vocab_size)\n",
    "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
    "            total_count += np.multiply(*data.size())\n",
    "            total_loss += loss.item() * np.multiply(*data.size()) # loss is averged by seq_length * batch_size\n",
    "           \n",
    "            \n",
    "    model.train()\n",
    "    return total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5) # every time we call this, learning rate reduces by 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "GRAD_CLIP = 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 10.822580337524414\n",
      "epoch 0 loss 4.204793930053711\n",
      "epoch 0 loss 2.6991500854492188\n",
      "epoch 0 loss 2.2078709602355957\n",
      "epoch 0 loss 1.6467599868774414\n",
      "epoch 0 loss 1.3863930702209473\n",
      "epoch 0 loss 1.1007905006408691\n",
      "epoch 0 loss 1.0194844007492065\n",
      "epoch 1 loss 0.8709428906440735\n",
      "epoch 1 loss 0.8289773464202881\n",
      "epoch 1 loss 0.77018141746521\n",
      "epoch 1 loss 0.7213648557662964\n",
      "epoch 1 loss 0.6132504343986511\n",
      "epoch 1 loss 0.5537371635437012\n",
      "epoch 1 loss 0.5588538646697998\n",
      "epoch 1 loss 0.5723716616630554\n"
     ]
    }
   ],
   "source": [
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    it = iter(train_iter)\n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    for i, batch in enumerate(it):\n",
    "        data, target = batch.text, batch.target # data: (seq_length, batch_size)\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden) # output: (seq_len, batch_size, vocab_size)\n",
    "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1)) # adjust shape based on the requirements of cross entropy loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP) # apply gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch\", epoch, \"loss\", loss.item())\n",
    "            \n",
    "        # save model + learning rate decay\n",
    "#         if i % 10000 == 0:\n",
    "#             val_loss = evaluate(model, val_iter)\n",
    "#             if len(val_losses) == 0 or val_loss < min(val_losses):\n",
    "#                 torch.save(model.state_dict(), \"lm.pth\")\n",
    "#                 print(\"best model saved to lm.pth\")\n",
    "#             else:\n",
    "#                 # learning rate decay\n",
    "#                 scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load model\n",
    "# best_model = RNNModel(vocab_size = len(TEXT.vocab), \n",
    "#                  embed_size = EMBEDDING_SIZE, \n",
    "#                  hidden_size = HIDDEN_SIZE, \n",
    "#                  nlayers = NLAYERS, \n",
    "#                  bidirectional=True, \n",
    "#                  dropout=0.3)\n",
    "# if USE_CUDA:\n",
    "#     best_model = best_model.to(device)\n",
    "    \n",
    "# best_model.load_state_dict(torch.load(\"lm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33034970410389697"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extensive labour formal labour precursors labour fine labour fine labour fine taj fine meat fine meat fine meat fine meat fine meat fine meat fine meat fine meat fine meat drug meat drug meat drug meat drug meat drug meat drug meat drug meat drug meat drug meat courage meat timbre meat lesion meat latex proposing latex proposing latex proposing but proposing but proposing but proposing but proposing but proposing but proposing but proposing but deuterium but sorceress but underway but underway but underway but underway but react but react but react but react but react but react but react\n"
     ]
    }
   ],
   "source": [
    "hidden = model.init_hidden(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
    "words = []\n",
    "for i in range(100):\n",
    "    output, hidden = model(inputs, hidden) # output: (1,1, vocab_size)\n",
    "    word_weights = output.squeeze().exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0] # based on the weights of each word, use multinomial distribution to select next work. Greedy serach using torch.max is deterministic which is used more often in translation\n",
    "    inputs.fill_(word_idx)\n",
    "    word = TEXT.vocab.itos[word_idx]\n",
    "    words.append(word)\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
